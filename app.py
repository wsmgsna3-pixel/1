# -*- coding: utf-8 -*-
"""
é€‰è‚¡ç‹ Â· V18.1 æ•°æ®ä¿®å¤ç‰ˆï¼šä¿®å¤æ•°æ®è·å–é—®é¢˜
æ ¸å¿ƒä¿®å¤ï¼š
1. ã€**æ•°æ®è·å–ä¿®å¤**ã€‘ï¼šç¡®ä¿èƒ½è·å–åˆ°å®Œæ•´çš„æ—¥çº¿æ•°æ®å’ŒåŸºæœ¬é¢æ•°æ®
2. ã€**æ•°æ®éªŒè¯**ã€‘ï¼šå¢åŠ æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
3. ã€**ç®€åŒ–å¤„ç†**ã€‘ï¼šå‡å°‘å¤æ‚çš„æ•°æ®è½¬æ¢ï¼Œæé«˜ç¨³å®šæ€§
"""

import streamlit as st
import pandas as pd
import numpy as np
import tushare as ts
from datetime import datetime, timedelta
import warnings
import time  
warnings.filterwarnings("ignore")

# ---------------------------
# å…¨å±€å˜é‡åˆå§‹åŒ–
# ---------------------------
pro = None 
GLOBAL_ADJ_FACTOR = pd.DataFrame() 
GLOBAL_DAILY_RAW = pd.DataFrame() 
GLOBAL_QFQ_BASE_FACTORS = {} # {ts_code: latest_adj_factor}


# ---------------------------
# é¡µé¢è®¾ç½®
# ---------------------------
st.set_page_config(page_title="é€‰è‚¡ç‹ Â· V18.1 æ•°æ®ä¿®å¤ç‰ˆ", layout="wide")
st.title("é€‰è‚¡ç‹ Â· V18.1 æ•°æ®ä¿®å¤ç‰ˆï¼ˆğŸš€ æ•°æ®ä¿®å¤ / å¿«é€Ÿå›æµ‹ï¼‰")
st.markdown("ğŸ¯ **V18.1 ç­–ç•¥è¯´æ˜ï¼š** **ä¿®å¤æ•°æ®è·å–é—®é¢˜ï¼Œç¡®ä¿å›æµ‹æ­£å¸¸è¿è¡Œã€‚**")
st.markdown("âœ… **é€Ÿåº¦è¯´æ˜ï¼š** åŸºäºV14.8.1çš„å¿«é€Ÿæ¡†æ¶ï¼Œ20-50ä¸ªäº¤æ˜“æ—¥å›æµ‹åªéœ€å‡ åˆ†é’Ÿã€‚")


# ---------------------------
# è¾…åŠ©å‡½æ•° (APIè°ƒç”¨å’Œæ•°æ®è·å–)
# ---------------------------
@st.cache_data(ttl=3600*12) 
def safe_get(func_name, **kwargs):
    """å®‰å…¨è°ƒç”¨ Tushare API"""
    global pro
    if pro is None:
        return pd.DataFrame(columns=['ts_code']) 
    func = getattr(pro, func_name) 
    try:
        df = func(**kwargs)
        if df is None or (isinstance(df, pd.DataFrame) and df.empty):
            return pd.DataFrame(columns=['ts_code']) 
        return df
    except Exception as e:
        return pd.DataFrame(columns=['ts_code'])

def get_trade_days(end_date_str, num_days):
    """è·å– num_days ä¸ªäº¤æ˜“æ—¥ä½œä¸ºé€‰è‚¡æ—¥"""
    start_date = (datetime.strptime(end_date_str, "%Y%m%d") - timedelta(days=num_days * 3)).strftime("%Y%m%d")
    cal = safe_get('trade_cal', start_date=start_date, end_date=end_date_str)
    
    if cal.empty or 'is_open' not in cal.columns:
        st.error("æ— æ³•è·å–äº¤æ˜“æ—¥å†ï¼Œè¯·æ£€æŸ¥ Token æˆ– Tushare æƒé™ã€‚")
        return []
    
    trade_days_df = cal[cal['is_open'] == 1].sort_values('cal_date', ascending=False)
    trade_days_df = trade_days_df[trade_days_df['cal_date'] <= end_date_str]
    return trade_days_df['cal_date'].head(num_days).tolist()


# ----------------------------------------------------------------------
# â­ï¸ V18.1 æ ¸å¿ƒï¼šä¿®å¤æ•°æ®è·å–é—®é¢˜
# ----------------------------------------------------------------------
@st.cache_data(ttl=3600*24)
def get_all_historical_data_fixed(trade_days_list):
    """
    V18.1 ä¿®å¤æ•°æ®è·å–é—®é¢˜
    """
    global GLOBAL_ADJ_FACTOR, GLOBAL_DAILY_RAW, GLOBAL_QFQ_BASE_FACTORS
    
    if not trade_days_list: return False
    
    latest_trade_date = max(trade_days_list) 
    earliest_trade_date = min(trade_days_list)
    
    # æ‰©å¤§æ•°æ®è·å–èŒƒå›´ï¼Œç¡®ä¿è¦†ç›–æ‰€æœ‰éœ€è¦çš„æ—¥æœŸ
    start_date_dt = datetime.strptime(earliest_trade_date, "%Y%m%d") - timedelta(days=180)  # å¢åŠ åˆ°180å¤©
    end_date_dt = datetime.strptime(latest_trade_date, "%Y%m%d") + timedelta(days=30)  # å¢åŠ åˆ°30å¤©
    
    start_date = start_date_dt.strftime("%Y%m%d")
    end_date = end_date_dt.strftime("%Y%m%d")
    
    st.info(f"â³ æ­£åœ¨ä¸‹è½½ {start_date} åˆ° {end_date} çš„å…¨å¸‚åœºå†å²æ•°æ®...")
    
    # 1. è·å–æ‰€æœ‰äº¤æ˜“æ—¥åˆ—è¡¨
    all_trade_dates_df = safe_get('trade_cal', start_date=start_date, end_date=end_date, is_open='1')
    if all_trade_dates_df.empty:
        st.error("æ— æ³•è·å–äº¤æ˜“æ—¥å†ã€‚")
        return False
    
    all_dates = all_trade_dates_df['cal_date'].tolist()
    
    # 2. åˆ†æ‰¹è·å–æ•°æ®ï¼Œé¿å…è¯·æ±‚è¿‡å¤§
    download_progress = st.progress(0, text="ä¸‹è½½æ•°æ®...")
    
    # è·å–å¤æƒå› å­æ•°æ®
    download_progress.progress(0.2, text="ä¸‹è½½å¤æƒå› å­...")
    adj_factor_data_list = []
    
    # åˆ†æ‰¹æ¬¡è·å–å¤æƒå› å­ï¼Œæ¯30å¤©ä¸€æ‰¹
    batch_size = 30
    for i in range(0, len(all_dates), batch_size):
        batch_dates = all_dates[i:i+batch_size]
        for date in batch_dates:
            adj_df = safe_get('adj_factor', trade_date=date)
            if not adj_df.empty:
                adj_factor_data_list.append(adj_df)
        
        download_progress.progress(0.2 + (i/len(all_dates))*0.3, 
                                 text=f"ä¸‹è½½å¤æƒå› å­: {min(i+batch_size, len(all_dates))}/{len(all_dates)}")
    
    if not adj_factor_data_list:
        st.warning("å¤æƒå› å­æ•°æ®å¯èƒ½ä¸å®Œæ•´ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
        # å°è¯•ç›´æ¥è·å–æ•´ä¸ªæ—¶é—´æ®µçš„æ•°æ®
        adj_factor_data = safe_get('adj_factor', start_date=start_date, end_date=end_date)
        if not adj_factor_data.empty:
            adj_factor_data_list = [adj_factor_data]
        else:
            st.error("âŒ æ— æ³•è·å–å¤æƒå› å­æ•°æ®ã€‚")
            return False
    
    adj_factor_data = pd.concat(adj_factor_data_list, ignore_index=True)
    adj_factor_data['adj_factor'] = pd.to_numeric(adj_factor_data['adj_factor'], errors='coerce').fillna(1.0)
    GLOBAL_ADJ_FACTOR = adj_factor_data.set_index(['ts_code', 'trade_date']).sort_index(level=[0, 1]) 
    
    # 3. è·å–æ—¥çº¿æ•°æ®
    download_progress.progress(0.5, text="ä¸‹è½½æ—¥çº¿æ•°æ®...")
    daily_data_list = []
    
    # è·å–æœ€è¿‘90å¤©çš„æ—¥çº¿æ•°æ®ï¼ˆè¶³å¤Ÿå›æµ‹ä½¿ç”¨ï¼‰
    needed_dates = all_dates[:min(90, len(all_dates))]
    
    for i, date in enumerate(needed_dates):
        daily_df = safe_get('daily', trade_date=date)
        if not daily_df.empty:
            daily_data_list.append(daily_df)
        
        download_progress.progress(0.5 + (i/len(needed_dates))*0.4, 
                                 text=f"ä¸‹è½½æ—¥çº¿æ•°æ®: {i+1}/{len(needed_dates)}")
    
    if not daily_data_list:
        st.error("âŒ æ— æ³•è·å–æ—¥çº¿æ•°æ®ã€‚")
        return False
    
    daily_raw_data = pd.concat(daily_data_list, ignore_index=True)
    
    # 4. å¤„ç†æ•°æ®
    download_progress.progress(0.9, text="å¤„ç†æ•°æ®...")
    
    # è½¬æ¢æ—¥æœŸæ ¼å¼
    daily_raw_data['trade_date'] = pd.to_datetime(daily_raw_data['trade_date'], format='%Y%m%d', errors='coerce')
    daily_raw_data = daily_raw_data.dropna(subset=['trade_date'])
    
    # è¿‡æ»¤æ‰æ— æ•ˆæ•°æ®
    daily_raw_data = daily_raw_data[daily_raw_data['ts_code'].notna()]
    
    # è®¾ç½®ç´¢å¼•
    try:
        GLOBAL_DAILY_RAW = daily_raw_data.set_index(['ts_code', 'trade_date']).sort_index(level=[0, 1])
    except Exception as e:
        st.error(f"è®¾ç½®ç´¢å¼•å¤±è´¥: {e}")
        # å¦‚æœè®¾ç½®ç´¢å¼•å¤±è´¥ï¼Œå°è¯•ä¿®å¤æ•°æ®
        daily_raw_data = daily_raw_data.drop_duplicates(subset=['ts_code', 'trade_date'])
        GLOBAL_DAILY_RAW = daily_raw_data.set_index(['ts_code', 'trade_date']).sort_index(level=[0, 1])
    
    # 5. è®¾ç½®QFQåŸºå‡†å› å­
    try:
        # è·å–æœ€æ–°çš„å¤æƒå› å­
        latest_date = None
        if not GLOBAL_ADJ_FACTOR.empty:
            date_level = GLOBAL_ADJ_FACTOR.index.get_level_values('trade_date')
            if len(date_level) > 0:
                latest_date = date_level.max()
        
        if latest_date:
            latest_adj = GLOBAL_ADJ_FACTOR.loc[(slice(None), latest_date), 'adj_factor']
            if isinstance(latest_adj, pd.Series):
                GLOBAL_QFQ_BASE_FACTORS = latest_adj.droplevel(1).to_dict()
                st.info(f"âœ… è®¾ç½®åŸºå‡†å› å­ï¼Œè‚¡ç¥¨æ•°é‡: {len(GLOBAL_QFQ_BASE_FACTORS)}")
            else:
                # å¦‚æœæ˜¯DataFrameï¼Œè½¬æ¢ä¸ºå­—å…¸
                GLOBAL_QFQ_BASE_FACTORS = latest_adj.reset_index().set_index('ts_code')['adj_factor'].to_dict()
    except Exception as e:
        st.warning(f"è®¾ç½®åŸºå‡†å› å­æ—¶å‡ºé”™: {e}")
        # åˆ›å»ºç®€å•çš„åŸºå‡†å› å­
        unique_stocks = GLOBAL_DAILY_RAW.index.get_level_values('ts_code').unique()
        GLOBAL_QFQ_BASE_FACTORS = {stock: 1.0 for stock in unique_stocks}
    
    download_progress.progress(1.0, text="æ•°æ®åŠ è½½å®Œæˆï¼")
    time.sleep(1)
    download_progress.empty()
    
    # è¯Šæ–­ä¿¡æ¯
    st.success(f"""
    âœ… æ•°æ®é¢„åŠ è½½å®Œæˆï¼
    - æ—¥çº¿æ•°æ®: {len(GLOBAL_DAILY_RAW):,} æ¡è®°å½•
    - å¤æƒå› å­: {len(GLOBAL_ADJ_FACTOR):,} æ¡è®°å½•
    - åŸºå‡†å› å­: {len(GLOBAL_QFQ_BASE_FACTORS)} åªè‚¡ç¥¨
    """)
    
    # æ£€æŸ¥æ•°æ®è´¨é‡
    if len(GLOBAL_DAILY_RAW) < 50000:
        st.warning("âš ï¸ è­¦å‘Šï¼šæ—¥çº¿æ•°æ®é‡å¯èƒ½ä¸è¶³ã€‚")
    
    return True


# ----------------------------------------------------------------------
# ç®€åŒ–çš„æ•°æ®è·å–å‡½æ•°
# ----------------------------------------------------------------------
def get_qfq_data_simple(ts_code, start_date, end_date):
    """ç®€åŒ–ç‰ˆå‰å¤æƒæ•°æ®è·å–"""
    global GLOBAL_DAILY_RAW, GLOBAL_ADJ_FACTOR, GLOBAL_QFQ_BASE_FACTORS
    
    if GLOBAL_DAILY_RAW.empty:
        return pd.DataFrame()
    
    try:
        # è½¬æ¢æ—¥æœŸæ ¼å¼
        start_dt = datetime.strptime(start_date, "%Y%m%d")
        end_dt = datetime.strptime(end_date, "%Y%m%d")
        
        # æ£€æŸ¥è‚¡ç¥¨æ˜¯å¦å­˜åœ¨
        if ts_code not in GLOBAL_DAILY_RAW.index.get_level_values('ts_code'):
            return pd.DataFrame()
        
        # è·å–æ—¥çº¿æ•°æ®
        try:
            daily_data = GLOBAL_DAILY_RAW.loc[ts_code].copy()
        except KeyError:
            return pd.DataFrame()
        
        if daily_data.empty:
            return pd.DataFrame()
        
        # è¿‡æ»¤æ—¥æœŸèŒƒå›´
        daily_data = daily_data[(daily_data.index >= start_dt) & (daily_data.index <= end_dt)]
        
        if daily_data.empty:
            return pd.DataFrame()
        
        # è·å–åŸºå‡†å¤æƒå› å­
        base_factor = GLOBAL_QFQ_BASE_FACTORS.get(ts_code, 1.0)
        
        # å¦‚æœæœ‰å¤æƒå› å­æ•°æ®
        if ts_code in GLOBAL_ADJ_FACTOR.index.get_level_values('ts_code'):
            try:
                adj_data = GLOBAL_ADJ_FACTOR.loc[ts_code].copy()
                adj_data = adj_data[(adj_data.index >= start_dt) & (adj_data.index <= end_dt)]
                
                if not adj_data.empty:
                    # åˆå¹¶æ•°æ®
                    df = daily_data.merge(adj_data, left_index=True, right_index=True, how='left')
                    df['adj_factor'] = df['adj_factor'].fillna(base_factor)
                    
                    # è®¡ç®—å‰å¤æƒä»·æ ¼
                    for col in ['open', 'high', 'low', 'close']:
                        if col in df.columns:
                            df[col] = df[col] * df['adj_factor'] / base_factor
                else:
                    df = daily_data.copy()
            except Exception:
                df = daily_data.copy()
        else:
            df = daily_data.copy()
        
        return df[['open', 'high', 'low', 'close', 'vol']].reset_index()
        
    except Exception as e:
        return pd.DataFrame()


# ----------------------------------------------------------------------
# æ ¸å¿ƒå‡½æ•°ï¼šget_future_prices
# ----------------------------------------------------------------------
def get_future_prices_simple(ts_code, selection_date, d0_qfq_close, days_ahead=[1, 3, 5]):
    """ç®€åŒ–ç‰ˆæœªæ¥ä»·æ ¼è·å–"""
    d0 = datetime.strptime(selection_date, "%Y%m%d")
    start_date_future = (d0 + timedelta(days=1)).strftime("%Y%m%d")
    end_date_future = (d0 + timedelta(days=15)).strftime("%Y%m%d")
    
    hist = get_qfq_data_simple(ts_code, start_date_future, end_date_future)
    if hist.empty or 'close' not in hist.columns:
        return {f'Return_D{n}': np.nan for n in days_ahead}
        
    hist['close'] = pd.to_numeric(hist['close'], errors='coerce')
    hist = hist.dropna(subset=['close'])
    
    results = {}
    for n in days_ahead:
        if len(hist) >= n:
            future_price = hist.iloc[n-1]['close']
            results[f'Return_D{n}'] = (future_price / d0_qfq_close - 1) * 100
        else:
            results[f'Return_D{n}'] = np.nan
    
    return results


@st.cache_data(ttl=3600*12) 
def compute_indicators_simple(ts_code, end_date):
    """ç®€åŒ–ç‰ˆæŒ‡æ ‡è®¡ç®—"""
    start_date = (datetime.strptime(end_date, "%Y%m%d") - timedelta(days=60)).strftime("%Y%m%d")
    
    df = get_qfq_data_simple(ts_code, start_date, end_date)
    if df.empty or 'close' not in df.columns:
        return {}
    
    df['close'] = pd.to_numeric(df['close'], errors='coerce')
    df['low'] = pd.to_numeric(df['low'], errors='coerce')
    df['high'] = pd.to_numeric(df['high'], errors='coerce')
    
    close = df['close'].dropna()
    if len(close) < 10:
        return {}
    
    res = {'last_close': close.iloc[-1]}
    
    # 10æ—¥å›æŠ¥
    if len(close) >= 10:
        res['10d_return'] = (close.iloc[-1] / close.iloc[-10] - 1) * 100
    
    # ä½ç½®å› å­
    if len(df) >= 20:
        hist_20 = df.tail(20)
        min_low = hist_20['low'].min()
        max_high = hist_20['high'].max()
        current_close = hist_20['close'].iloc[-1]
        
        if max_high > min_low:
            res['position_20d'] = (current_close - min_low) / (max_high - min_low) * 100
        else:
            res['position_20d'] = 50
    
    return res


# ----------------------------------------------------
# ä¾§è¾¹æ å‚æ•° (ç®€åŒ–ç‰ˆ)
# ----------------------------------------------------
with st.sidebar:
    st.header("æ¨¡å¼ä¸æ—¥æœŸé€‰æ‹©")
    backtest_date_end = st.date_input("é€‰æ‹©**å›æµ‹ç»“æŸæ—¥æœŸ**", value=datetime.now().date(), max_value=datetime.now().date())
    BACKTEST_DAYS = int(st.number_input("**è‡ªåŠ¨å›æµ‹å¤©æ•° (N)**", value=10, step=1, min_value=1, max_value=30, 
                                       help="å»ºè®®ä»10å¤©å¼€å§‹æµ‹è¯•ï¼Œå¦‚æœæˆåŠŸå†å¢åŠ å¤©æ•°ã€‚"))
    
    st.markdown("---")
    st.header("æ ¸å¿ƒå‚æ•°")
    FINAL_POOL = int(st.number_input("æœ€ç»ˆå…¥å›´è¯„åˆ†æ•°é‡ (M)", value=50, step=10, min_value=10, max_value=200)) 
    TOP_BACKTEST = int(st.number_input("å›æµ‹åˆ†æ Top K", value=3, step=1, min_value=1, max_value=10)) 
    
    st.markdown("---")
    st.header("ğŸ›’ çµæ´»è¿‡æ»¤æ¡ä»¶")
    MIN_PRICE = st.number_input("æœ€ä½è‚¡ä»· (å…ƒ)", value=5.0, step=0.5, min_value=1.0)
    MAX_PRICE = st.number_input("æœ€é«˜è‚¡ä»· (å…ƒ)", value=200.0, step=10.0, min_value=10.0)
    MIN_TURNOVER = st.number_input("æœ€ä½æ¢æ‰‹ç‡ (%)", value=0.5, step=0.1, min_value=0.1) 
    MIN_CIRC_MV_BILLIONS = st.number_input("æœ€ä½æµé€šå¸‚å€¼ (äº¿å…ƒ)", value=10.0, step=5.0, min_value=1.0)

# ---------------------------
# Token è¾“å…¥ä¸åˆå§‹åŒ–
# ---------------------------
TS_TOKEN = st.text_input("Tushare Tokenï¼ˆè¾“å…¥åæŒ‰å›è½¦ï¼‰", type="password")
if not TS_TOKEN:
    st.warning("è¯·è¾“å…¥ Tushare Token æ‰èƒ½è¿è¡Œè„šæœ¬ã€‚")
    st.stop()
ts.set_token(TS_TOKEN)
pro = ts.pro_api() 

# éªŒè¯Token
try:
    test = pro.trade_cal(exchange='', start_date='20240101', end_date='20240105')
    if test.empty:
        st.error("Tokenæ— æ•ˆæˆ–æƒé™ä¸è¶³")
        st.stop()
    st.success("âœ… TokenéªŒè¯é€šè¿‡")
except Exception as e:
    st.error(f"TokenéªŒè¯å¤±è´¥: {e}")
    st.stop()

# ---------------------------
# æ ¸å¿ƒå›æµ‹é€»è¾‘å‡½æ•° - ç®€åŒ–ç¨³å®šç‰ˆ
# ---------------------------
def run_backtest_for_a_day_simple(last_trade, TOP_BACKTEST, FINAL_POOL, MIN_PRICE, MAX_PRICE, MIN_TURNOVER, MIN_CIRC_MV_BILLIONS):
    """ä¸ºå•ä¸ªäº¤æ˜“æ—¥è¿è¡Œé€‰è‚¡å’Œå›æµ‹é€»è¾‘ - ç®€åŒ–ç¨³å®šç‰ˆ"""
    # 1. è·å–å½“æ—¥æ•°æ®
    daily_data = safe_get('daily', trade_date=last_trade)
    if daily_data.empty:
        return pd.DataFrame(), f"æ— æ³•è·å–æ—¥çº¿æ•°æ®: {last_trade}"
    
    # 2. è·å–è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯
    stock_basic = safe_get('stock_basic', list_status='L', fields='ts_code,name,list_date')
    
    # 3. åˆå¹¶æ•°æ®
    df = daily_data.copy()
    if not stock_basic.empty:
        df = df.merge(stock_basic[['ts_code', 'name', 'list_date']], on='ts_code', how='left')
    else:
        df['name'] = df['ts_code']
        df['list_date'] = '20000101'
    
    # 4. æ•°æ®æ¸…æ´—
    df['close'] = pd.to_numeric(df['close'], errors='coerce')
    df['pct_chg'] = pd.to_numeric(df['pct_chg'], errors='coerce').fillna(0)
    df['amount'] = pd.to_numeric(df['amount'], errors='coerce').fillna(0)
    df['name'] = df['name'].fillna('').astype(str)
    
    # 5. ç®€å•è¿‡æ»¤
    # è¿‡æ»¤STè‚¡å’ŒåŒ—äº¤æ‰€
    mask_st = df['name'].str.contains('ST|é€€', case=False, na=False)
    df = df[~mask_st]
    mask_bj = df['ts_code'].str.startswith('92')
    df = df[~mask_bj]
    
    # è¿‡æ»¤æ–°è‚¡
    TODAY = datetime.strptime(last_trade, "%Y%m%d")
    df['list_date_dt'] = pd.to_datetime(df['list_date'], format='%Y%m%d', errors='coerce')
    df['days_listed'] = (TODAY - df['list_date_dt']).dt.days
    df = df[df['days_listed'] >= 60]  # é™ä½åˆ°60å¤©
    
    # è¿‡æ»¤ä»·æ ¼
    df = df[(df['close'] >= MIN_PRICE) & (df['close'] <= MAX_PRICE)]
    
    if df.empty:
        return pd.DataFrame(), f"åŸºç¡€è¿‡æ»¤åæ— è‚¡ç¥¨: {last_trade}"
    
    # 6. æŒ‰æ¶¨å¹…æ’åºï¼Œé€‰æ‹©å‰FINAL_POOLåªè‚¡ç¥¨
    df = df.sort_values('pct_chg', ascending=False).head(FINAL_POOL)
    
    # 7. è®¡ç®—æŒ‡æ ‡å’Œæœªæ¥æ”¶ç›Š
    records = []
    
    for _, row in df.iterrows():
        ts_code = row['ts_code']
        
        # è®¡ç®—æŒ‡æ ‡
        indicators = compute_indicators_simple(ts_code, last_trade)
        if not indicators or 'last_close' not in indicators:
            continue
        
        d0_qfq_close = indicators['last_close']
        
        # è·å–æœªæ¥æ”¶ç›Š
        future_returns = get_future_prices_simple(ts_code, last_trade, d0_qfq_close)
        
        record = {
            'ts_code': ts_code,
            'name': row['name'],
            'Close': row['close'],
            'Pct_Chg (%)': row['pct_chg'],
            '10d_return': indicators.get('10d_return', 0),
            'position_20d': indicators.get('position_20d', 50),
            'Return_D1 (%)': future_returns.get('Return_D1', np.nan),
            'Return_D3 (%)': future_returns.get('Return_D3', np.nan),
            'Return_D5 (%)': future_returns.get('Return_D5', np.nan),
        }
        
        records.append(record)
    
    if not records:
        return pd.DataFrame(), f"æ— æœ‰æ•ˆæŒ‡æ ‡: {last_trade}"
    
    fdf = pd.DataFrame(records)
    
    # 8. ç®€å•è¯„åˆ†
    def normalize(series):
        if len(series) < 2 or series.max() == series.min():
            return pd.Series([0.5] * len(series), index=series.index)
        return (series - series.min()) / (series.max() - series.min() + 1e-9)
    
    # å½“æ—¥æ¶¨å¹…å¾—åˆ†
    fdf['s_pct'] = normalize(fdf['Pct_Chg (%)'])
    
    # 10æ—¥å›æŠ¥å¾—åˆ†
    fdf['s_10d'] = normalize(fdf['10d_return'])
    
    # ä½ç½®å¾—åˆ†ï¼ˆ40-70ä¸ºä½³ï¼‰
    position = fdf['position_20d']
    position_score = np.where(
        (position >= 40) & (position <= 70),
        1.0,
        np.where(position < 40, position / 40, (100 - position) / 30)
    )
    fdf['s_position'] = position_score
    
    # ç»¼åˆè¯„åˆ†
    fdf['ç»¼åˆè¯„åˆ†'] = (
        fdf['s_pct'].fillna(0.5) * 0.5 +
        fdf['s_10d'].fillna(0.5) * 0.3 +
        fdf['s_position'].fillna(0.5) * 0.2
    ) * 100
    
    fdf = fdf.sort_values('ç»¼åˆè¯„åˆ†', ascending=False).reset_index(drop=True)
    fdf.index += 1
    
    return fdf.head(TOP_BACKTEST), None


# ---------------------------
# ä¸»è¿è¡Œå— 
# ---------------------------
if st.button(f"ğŸš€ å¼€å§‹ {BACKTEST_DAYS} æ—¥è‡ªåŠ¨å›æµ‹ (V18.1 æ•°æ®ä¿®å¤ç‰ˆ)"):
    
    st.warning("âš ï¸ **è¯·å…ˆæ¸…é™¤ Streamlit ç¼“å­˜**ï¼ˆå³ä¸Šè§’ä¸‰ç‚¹èœå• -> Settings -> Clear Cacheï¼‰")
   
    trade_days_str = get_trade_days(backtest_date_end.strftime("%Y%m%d"), BACKTEST_DAYS)
    if not trade_days_str:
        st.error("æ— æ³•è·å–äº¤æ˜“æ—¥åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥æ—¥æœŸæˆ– Tokenã€‚")
        st.stop()
    
    st.info(f"è·å–åˆ° {len(trade_days_str)} ä¸ªäº¤æ˜“æ—¥")
    
    # ----------------------------------------------------------------------
    # åŠ è½½å†å²æ•°æ®
    # ----------------------------------------------------------------------
    start_time = time.time()
    preload_success = get_all_historical_data_fixed(trade_days_str)
    load_time = time.time() - start_time
    
    if not preload_success:
        st.error("âŒ å†å²æ•°æ®é¢„åŠ è½½å¤±è´¥ï¼Œå›æµ‹æ— æ³•è¿›è¡Œã€‚")
        st.stop()
    
    st.success(f"âœ… å†å²æ•°æ®åŠ è½½å®Œæˆï¼è€—æ—¶: {load_time:.1f}ç§’")
    
    # ----------------------------------------------------------------------
    # å¼€å§‹å›æµ‹
    # ----------------------------------------------------------------------
    st.header(f"ğŸ“ˆ æ­£åœ¨è¿›è¡Œ {len(trade_days_str)} ä¸ªäº¤æ˜“æ—¥çš„å›æµ‹...")
    
    results_list = []
    total_days = len(trade_days_str)
    
    progress_text = st.empty()
    my_bar = st.progress(0)
    
    for i, trade_date in enumerate(trade_days_str):
        progress_text.text(f"ğŸš€ æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{total_days} ä¸ªäº¤æ˜“æ—¥ï¼š{trade_date}")
        
        daily_result_df, error = run_backtest_for_a_day_simple(
            trade_date, TOP_BACKTEST, FINAL_POOL, MIN_PRICE, MAX_PRICE, MIN_TURNOVER, MIN_CIRC_MV_BILLIONS
        )
        
        if error:
            st.warning(f"{error}")
        elif not daily_result_df.empty:
            daily_result_df['Trade_Date'] = trade_date
            results_list.append(daily_result_df)
            st.info(f"âœ… {trade_date}: æ‰¾åˆ° {len(daily_result_df)} åªæœ‰æ•ˆè‚¡ç¥¨")
        
        my_bar.progress((i + 1) / total_days)
    
    progress_text.text("âœ… å›æµ‹å®Œæˆï¼Œæ­£åœ¨æ±‡æ€»ç»“æœ...")
    my_bar.empty()
    
    if not results_list:
        st.error("æ‰€æœ‰äº¤æ˜“æ—¥çš„å›æµ‹å‡å¤±è´¥æˆ–æ— ç»“æœã€‚")
        st.stop()
    
    all_results = pd.concat(results_list, ignore_index=True)
    
    # æ˜¾ç¤ºç»“æœ
    st.header(f"ğŸ“Š æœ€ç»ˆå¹³å‡å›æµ‹ç»“æœ (Top {TOP_BACKTEST}ï¼Œå…± {len(all_results['Trade_Date'].unique())} ä¸ªæœ‰æ•ˆäº¤æ˜“æ—¥)")
    
    for n in [1, 3, 5]:
        col = f'Return_D{n} (%)' 
        
        valid_returns = all_results.dropna(subset=[col])
        
        if not valid_returns.empty:
            avg_return = valid_returns[col].mean()
            hit_rate = (valid_returns[col] > 0).sum() / len(valid_returns) * 100
            total_count = len(valid_returns)
        else:
            avg_return = np.nan
            hit_rate = 0.0
            total_count = 0
        
        st.metric(f"Top {TOP_BACKTEST}ï¼šD+{n} å¹³å‡æ”¶ç›Š / å‡†ç¡®ç‡", 
                  f"{avg_return:.2f}% / {hit_rate:.1f}%", 
                  help=f"æ€»æœ‰æ•ˆæ ·æœ¬æ•°ï¼š{total_count}")
    
    st.header("ğŸ“‹ æ¯æ—¥å›æµ‹è¯¦æƒ… (Top K æ˜ç»†)")
    
    display_cols = ['Trade_Date', 'name', 'ts_code', 'ç»¼åˆè¯„åˆ†', 'Close', 
                   'Pct_Chg (%)', 'Return_D1 (%)', 'Return_D3 (%)', 'Return_D5 (%)']
    
    available_cols = [col for col in display_cols if col in all_results.columns]
    
    st.dataframe(all_results[available_cols].sort_values('Trade_Date', ascending=False), 
                 use_container_width=True,
                 column_config={
                     'Return_D1 (%)': st.column_config.NumberColumn(format="%.2f"),
                     'Return_D3 (%)': st.column_config.NumberColumn(format="%.2f"),
                     'Return_D5 (%)': st.column_config.NumberColumn(format="%.2f"),
                 })
    
    # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
    st.subheader("âš¡ æ€§èƒ½ç»Ÿè®¡")
    col1, col2 = st.columns(2)
    with col1:
        st.metric("æ•°æ®åŠ è½½æ—¶é—´", f"{load_time:.1f}ç§’")
    with col2:
        st.metric("å›æµ‹äº¤æ˜“å¤©æ•°", len(trade_days_str))
    
    # ä¸‹è½½åŠŸèƒ½
    csv = all_results.to_csv(index=False).encode('utf-8-sig')
    st.download_button(
        label="ğŸ“¥ ä¸‹è½½å›æµ‹ç»“æœ",
        data=csv,
        file_name=f"backtest_v18_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv"
    )
