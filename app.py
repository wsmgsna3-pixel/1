# -*- coding: utf-8 -*-
"""
é€‰è‚¡ç‹ Â· V30.12.13 ç¾è‚¡è‡ªåŠ¨æ˜ å°„ç‰ˆ
------------------------------------------------
æ–°å¢åŠŸèƒ½ï¼šã€ç¾è‚¡çƒ­ç‚¹è‡ªåŠ¨åŒæ­¥ã€‘
- é€»è¾‘ï¼šé€šè¿‡ yfinance è·å–ç¾è‚¡ 11 å¤§è¡Œä¸š ETF æ˜¨å¤œæ¶¨è·Œå¹…ã€‚
- è‡ªåŠ¨ï¼šå–æ¶¨å¹…å‰ 5 åï¼Œè‡ªåŠ¨æ˜ å°„å¹¶å‹¾é€‰ A è‚¡å¯¹åº”è¡Œä¸šã€‚
------------------------------------------------
"""

import streamlit as st
import pandas as pd
import numpy as np
import tushare as ts
from datetime import datetime, timedelta
import warnings
import time
import concurrent.futures 
import os
import pickle

# ã€æ–°å¢ã€‘å¼•å…¥ yfinance
try:
    import yfinance as yf
except ImportError:
    st.error("è¯·å…ˆå®‰è£… yfinance åº“: pip install yfinance")

warnings.filterwarnings("ignore")

# ---------------------------
# å…¨å±€å˜é‡
# ---------------------------
pro = None 
GLOBAL_ADJ_FACTOR = pd.DataFrame() 
GLOBAL_DAILY_RAW = pd.DataFrame() 
GLOBAL_QFQ_BASE_FACTORS = {} 
GLOBAL_STOCK_INDUSTRY = {} 

# ---------------------------
# ç¾è‚¡ ETF -> Aè‚¡ç”³ä¸‡è¡Œä¸š æ˜ å°„è¡¨
# ---------------------------
US_SECTOR_MAP = {
    'XLK': {'name': 'ç§‘æŠ€(XLK)', 'cn_inds': ['ç”µå­', 'è®¡ç®—æœº', 'é€šä¿¡']},
    'SOXX': {'name': 'åŠå¯¼ä½“(SOXX)', 'cn_inds': ['ç”µå­']}, # ç‰¹åˆ«åŠ å…¥åŠå¯¼ä½“
    'XLC': {'name': 'é€šä¿¡æœåŠ¡(XLC)', 'cn_inds': ['ä¼ åª’', 'é€šä¿¡']},
    'XLV': {'name': 'åŒ»è¯(XLV)', 'cn_inds': ['åŒ»è¯ç”Ÿç‰©', 'ç¾å®¹æŠ¤ç†']},
    'XLY': {'name': 'å¯é€‰æ¶ˆè´¹(XLY)', 'cn_inds': ['æ±½è½¦', 'å®¶ç”¨ç”µå™¨', 'å•†è´¸é›¶å”®', 'çººç»‡æœé¥°']},
    'XLP': {'name': 'å¿…é€‰æ¶ˆè´¹(XLP)', 'cn_inds': ['é£Ÿå“é¥®æ–™', 'å†œæ—ç‰§æ¸”']},
    'XLE': {'name': 'èƒ½æº(XLE)', 'cn_inds': ['çŸ³æ²¹çŸ³åŒ–', 'ç…¤ç‚­']},
    'XLF': {'name': 'é‡‘è(XLF)', 'cn_inds': ['é“¶è¡Œ', 'éé“¶é‡‘è']},
    'XLI': {'name': 'å·¥ä¸š(XLI)', 'cn_inds': ['æœºæ¢°è®¾å¤‡', 'ç”µåŠ›è®¾å¤‡', 'å›½é˜²å†›å·¥', 'å»ºç­‘è£…é¥°']},
    'XLB': {'name': 'ææ–™(XLB)', 'cn_inds': ['æœ‰è‰²é‡‘å±', 'åŸºç¡€åŒ–å·¥', 'é’¢é“', 'å»ºç­‘ææ–™']},
    'XLRE': {'name': 'æˆ¿åœ°äº§(XLRE)', 'cn_inds': ['æˆ¿åœ°äº§']},
    'XLU': {'name': 'å…¬ç”¨äº‹ä¸š(XLU)', 'cn_inds': ['å…¬ç”¨äº‹ä¸š', 'ç¯ä¿', 'ç”µåŠ›è®¾å¤‡']}
}

# ç”³ä¸‡ä¸€çº§è¡Œä¸šåˆ—è¡¨
SW_INDUSTRIES = {
    '801010.SI': 'å†œæ—ç‰§æ¸”', '801030.SI': 'åŸºç¡€åŒ–å·¥', '801040.SI': 'é’¢é“',
    '801050.SI': 'æœ‰è‰²é‡‘å±', '801080.SI': 'ç”µå­', '801710.SI': 'å»ºç­‘ææ–™',
    '801720.SI': 'å»ºç­‘è£…é¥°', '801730.SI': 'ç”µåŠ›è®¾å¤‡', '801740.SI': 'å›½é˜²å†›å·¥',
    '801750.SI': 'è®¡ç®—æœº', '801760.SI': 'ä¼ åª’', '801770.SI': 'é€šä¿¡',
    '801880.SI': 'æ±½è½¦', '801890.SI': 'æœºæ¢°è®¾å¤‡', '801090.SI': 'äº¤è¿è®¾å¤‡', 
    '801110.SI': 'å®¶ç”¨ç”µå™¨', '801120.SI': 'é£Ÿå“é¥®æ–™', '801130.SI': 'çººç»‡æœé¥°',
    '801140.SI': 'è½»å·¥åˆ¶é€ ', '801150.SI': 'åŒ»è¯ç”Ÿç‰©', '801160.SI': 'å…¬ç”¨äº‹ä¸š',
    '801170.SI': 'äº¤é€šè¿è¾“', '801180.SI': 'æˆ¿åœ°äº§', '801200.SI': 'å•†è´¸é›¶å”®',
    '801210.SI': 'ç¤¾ä¼šæœåŠ¡', '801230.SI': 'ç»¼åˆ', '801780.SI': 'é“¶è¡Œ',
    '801790.SI': 'éé“¶é‡‘è', '801950.SI': 'ç…¤ç‚­', '801960.SI': 'çŸ³æ²¹çŸ³åŒ–',
    '801970.SI': 'ç¯ä¿', '801980.SI': 'ç¾å®¹æŠ¤ç†'
}
SW_NAMES_LIST = list(SW_INDUSTRIES.values())

# ---------------------------
# é¡µé¢è®¾ç½®
# ---------------------------
st.set_page_config(page_title="é€‰è‚¡ç‹ V30.12.13 ç¾è‚¡è”åŠ¨ç‰ˆ", layout="wide")
st.title("é€‰è‚¡ç‹ V30.12.13ï¼šç¾è‚¡è”åŠ¨ç‰ˆ (è‡ªåŠ¨æ˜ å°„çƒ­ç‚¹)")

# ---------------------------
# åŸºç¡€ API å‡½æ•°
# ---------------------------
@st.cache_data(ttl=3600*12) 
def safe_get(func_name, **kwargs):
    global pro
    if pro is None: return pd.DataFrame(columns=['ts_code']) 
    func = getattr(pro, func_name) 
    try:
        for _ in range(3):
            try:
                if kwargs.get('is_index'): df = pro.index_daily(**kwargs)
                else: df = func(**kwargs)
                if df is not None and not df.empty: return df
                time.sleep(0.5)
            except:
                time.sleep(1)
                continue
        return pd.DataFrame(columns=['ts_code']) 
    except: return pd.DataFrame(columns=['ts_code'])

def get_trade_days(end_date_str, num_days):
    lookback_days = max(num_days * 3, 365) 
    start_date = (datetime.strptime(end_date_str, "%Y%m%d") - timedelta(days=lookback_days)).strftime("%Y%m%d")
    cal = safe_get('trade_cal', start_date=start_date, end_date=end_date_str)
    if cal.empty: return []
    trade_days_df = cal[cal['is_open'] == 1].sort_values('cal_date', ascending=False)
    return trade_days_df[trade_days_df['cal_date'] <= end_date_str]['cal_date'].head(num_days).tolist()

@st.cache_data(ttl=3600*24)
def fetch_and_cache_daily_data(date):
    adj = safe_get('adj_factor', trade_date=date)
    daily = safe_get('daily', trade_date=date)
    return {'adj': adj, 'daily': daily}

@st.cache_data(ttl=3600*24*7) 
def load_industry_mapping():
    global pro
    if pro is None: return {}
    try:
        all_members = []
        load_bar = st.progress(0, text="åŠ è½½è¡Œä¸šæ•°æ®...")
        codes = list(SW_INDUSTRIES.keys())
        for i, idx_code in enumerate(codes):
            df = pro.index_member(index_code=idx_code, is_new='Y')
            if not df.empty:
                df['industry_name'] = SW_INDUSTRIES[idx_code]
                all_members.append(df[['con_code', 'industry_name']])
            time.sleep(0.02)
            load_bar.progress((i + 1) / len(codes))
        load_bar.empty()
        if not all_members: return {}
        full_df = pd.concat(all_members)
        full_df = full_df.drop_duplicates(subset=['con_code'])
        return dict(zip(full_df['con_code'], full_df['industry_name']))
    except: return {}

# ---------------------------
# æ•°æ®ç¼“å­˜ä¸è·å–
# ---------------------------
CACHE_FILE_NAME = "market_data_cache.pkl"

def get_all_historical_data(trade_days_list, use_cache=True):
    global GLOBAL_ADJ_FACTOR, GLOBAL_DAILY_RAW, GLOBAL_QFQ_BASE_FACTORS, GLOBAL_STOCK_INDUSTRY
    
    GLOBAL_STOCK_INDUSTRY = load_industry_mapping()

    if use_cache and os.path.exists(CACHE_FILE_NAME):
        st.success("âš¡ æé€ŸåŠ è½½æœ¬åœ°ç¼“å­˜...")
        try:
            with open(CACHE_FILE_NAME, 'rb') as f:
                d = pickle.load(f)
                GLOBAL_ADJ_FACTOR = d['adj']
                GLOBAL_DAILY_RAW = d['daily']
            latest = GLOBAL_ADJ_FACTOR.index.get_level_values('trade_date').max()
            if latest:
                try: GLOBAL_QFQ_BASE_FACTORS = GLOBAL_ADJ_FACTOR.loc[(slice(None), latest), 'adj_factor'].droplevel(1).to_dict()
                except: pass
            return True
        except: os.remove(CACHE_FILE_NAME)

    latest_trade_date = max(trade_days_list) 
    earliest_trade_date = min(trade_days_list)
    start_date_dt = datetime.strptime(earliest_trade_date, "%Y%m%d") - timedelta(days=200)
    end_date_dt = datetime.strptime(latest_trade_date, "%Y%m%d") + timedelta(days=30)
    start_date = start_date_dt.strftime("%Y%m%d")
    end_date = end_date_dt.strftime("%Y%m%d")
    all_dates = safe_get('trade_cal', start_date=start_date, end_date=end_date, is_open='1')['cal_date'].tolist()
    
    st.info(f"ğŸ“¡ æ­£åœ¨ä¸‹è½½æ•°æ®: {start_date} è‡³ {end_date}...")
    adj_list, daily_list = [], []
    def fetch_worker(date): return fetch_and_cache_daily_data(date)
    bar = st.progress(0)
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        f2d = {executor.submit(fetch_worker, d): d for d in all_dates}
        for i, f in enumerate(concurrent.futures.as_completed(f2d)):
            try:
                d = f.result()
                if not d['adj'].empty: adj_list.append(d['adj'])
                if not d['daily'].empty: daily_list.append(d['daily'])
            except: pass
            if i%10==0: bar.progress((i+1)/len(all_dates))
    bar.empty()
    
    if not daily_list: return False
    with st.spinner("æ„å»ºç´¢å¼•..."):
        GLOBAL_ADJ_FACTOR = pd.concat(adj_list).drop_duplicates(subset=['ts_code', 'trade_date']).set_index(['ts_code', 'trade_date']).sort_index()
        GLOBAL_ADJ_FACTOR['adj_factor'] = pd.to_numeric(GLOBAL_ADJ_FACTOR['adj_factor'], errors='coerce').fillna(0)
        GLOBAL_DAILY_RAW = pd.concat(daily_list).drop_duplicates(subset=['ts_code', 'trade_date']).set_index(['ts_code', 'trade_date']).sort_index()
        
        latest = GLOBAL_ADJ_FACTOR.index.get_level_values('trade_date').max()
        if latest:
            try: GLOBAL_QFQ_BASE_FACTORS = GLOBAL_ADJ_FACTOR.loc[(slice(None), latest), 'adj_factor'].droplevel(1).to_dict()
            except: pass
            
        try:
            with open(CACHE_FILE_NAME, 'wb') as f: pickle.dump({'adj': GLOBAL_ADJ_FACTOR, 'daily': GLOBAL_DAILY_RAW}, f)
        except: pass
    return True

# ---------------------------
# æŒ‡æ ‡è®¡ç®—
# ---------------------------
def get_qfq_data_v4_optimized_final(ts_code, start_date, end_date):
    global GLOBAL_DAILY_RAW, GLOBAL_ADJ_FACTOR, GLOBAL_QFQ_BASE_FACTORS
    if GLOBAL_DAILY_RAW.empty: return pd.DataFrame()
    latest_adj = GLOBAL_QFQ_BASE_FACTORS.get(ts_code, np.nan)
    if pd.isna(latest_adj): return pd.DataFrame()
    try:
        d = GLOBAL_DAILY_RAW.loc[ts_code]
        d = d[(d.index >= start_date) & (d.index <= end_date)]
        a = GLOBAL_ADJ_FACTOR.loc[ts_code]['adj_factor']
        a = a[(a.index >= start_date) & (a.index <= end_date)]
    except: return pd.DataFrame()
    if d.empty or a.empty: return pd.DataFrame()
    df = d.merge(a.rename('adj_factor'), left_index=True, right_index=True, how='left').dropna()
    for c in ['open','high','low','close']: df[c+'_qfq'] = df[c] * df['adj_factor'] / latest_adj
    df = df.reset_index().rename(columns={'trade_date':'trade_date_str'}).sort_values('trade_date_str').set_index('trade_date_str')
    for c in ['open','high','low','close']: df[c] = df[c+'_qfq']
    return df[['open','high','low','close','vol']]

# ã€é‡è¦ã€‘æ¢å¤ä¸ºâ€œæ¡ä»¶ä¹°å…¥â€é€»è¾‘ (ç²¾è‹±ç‰ˆ)
def get_future_prices(ts_code, selection_date, d0_qfq_close, days_ahead=[1, 3, 5]):
    d0 = datetime.strptime(selection_date, "%Y%m%d")
    s = (d0 + timedelta(days=1)).strftime("%Y%m%d")
    e = (d0 + timedelta(days=15)).strftime("%Y%m%d")
    h = get_qfq_data_v4_optimized_final(ts_code, start_date=s, end_date=e)
    res = {}
    if h.empty: return res
    h['open'] = pd.to_numeric(h['open'], errors='coerce')
    h['high'] = pd.to_numeric(h['high'], errors='coerce')
    h['close'] = pd.to_numeric(h['close'], errors='coerce')
    
    d1 = h.iloc[0]
    next_open = d1['open']
    next_high = d1['high']
    
    # === ç²¾è‹±ç‰ˆä¹°å…¥æ¡ä»¶ ===
    if next_open <= d0_qfq_close: return res
    target_buy = next_open * 1.015
    if next_high < target_buy: return res
    
    for n in days_ahead:
        col = f'Return_D{n}'
        if len(h) >= n:
            res[col] = (h.iloc[n-1]['close'] - target_buy)/target_buy * 100
    return res

def compute_indicators(ts_code, end_date):
    s = (datetime.strptime(end_date, "%Y%m%d") - timedelta(days=150)).strftime("%Y%m%d")
    df = get_qfq_data_v4_optimized_final(ts_code, s, end_date)
    if df.empty or len(df)<26: return {}
    df['pct'] = df['close'].pct_change().fillna(0)*100
    res = {'last_close': df['close'].iloc[-1], 'last_high': df['high'].iloc[-1]}
    
    ema12 = df['close'].ewm(span=12).mean()
    ema26 = df['close'].ewm(span=26).mean()
    diff = ema12 - ema26
    dea = diff.ewm(span=9).mean()
    res['macd_val'] = ((diff-dea)*2).iloc[-1]
    
    ma20 = df['close'].tail(20).mean()
    res['ma20'] = ma20
    if ma20 > 0: res['bias_20'] = (res['last_close']-ma20)/ma20*100
    else: res['bias_20'] = 0
    
    delta = df['close'].diff()
    gain = (delta.where(delta>0, 0)).ewm(alpha=1/12).mean()
    loss = (-delta.where(delta<0, 0)).ewm(alpha=1/12).mean()
    rs = gain/(loss+1e-9)
    res['rsi_12'] = 100 - (100/(1+rs)).iloc[-1]
    return res

def get_market_state(trade_date):
    s = (datetime.strptime(trade_date, "%Y%m%d") - timedelta(days=40)).strftime("%Y%m%d")
    i = safe_get('daily', ts_code='000300.SH', start_date=s, end_date=trade_date, is_index=True)
    if i.empty or len(i)<20: return 'Weak'
    return 'Strong' if i.iloc[-1]['close'] > i['close'].tail(20).mean() else 'Weak'

# ---------------------------
# ã€æ ¸å¿ƒåŠŸèƒ½ã€‘è‡ªåŠ¨è·å–ç¾è‚¡æ•°æ®
# ---------------------------
def auto_get_us_hot_sectors():
    tickers = list(US_SECTOR_MAP.keys())
    try:
        # è·å–æœ€è¿‘ 5 å¤©æ•°æ®ä»¥è®¡ç®—æœ€æ–°æ¶¨è·Œå¹…
        data = yf.download(tickers, period="5d", progress=False)['Close']
        if data.empty: return [], "è·å–å¤±è´¥ï¼šæ•°æ®ä¸ºç©º"
        
        # è®¡ç®—æœ€åä¸€æ—¥æ¶¨è·Œå¹…
        pct_change = data.pct_change().iloc[-1] * 100
        pct_change = pct_change.sort_values(ascending=False)
        
        # å–å‰ 5 å
        top5 = pct_change.head(5)
        
        # æ˜ å°„å› A è‚¡è¡Œä¸š
        target_cn_inds = set()
        msg_lines = []
        msg_lines.append("ğŸ‡ºğŸ‡¸ æ˜¨å¤œç¾è‚¡çƒ­ç‚¹ (Top 5):")
        
        for etf_code, chg in top5.items():
            info = US_SECTOR_MAP.get(etf_code)
            if info:
                name = info['name']
                mapped = info['cn_inds']
                msg_lines.append(f"{name}: {chg:.2f}% -> ğŸ‡¨ğŸ‡³ {','.join(mapped)}")
                target_cn_inds.update(mapped)
                
        return list(target_cn_inds), "\n".join(msg_lines)
    except Exception as e:
        return [], f"è·å–å¤±è´¥: {str(e)}"

# ---------------------------
# å›æµ‹ä¸»é€»è¾‘
# ---------------------------
def run_backtest_for_a_day(last_trade, TOP_BACKTEST, FINAL_POOL, MAX_UPPER_SHADOW, MAX_TURNOVER_RATE, RSI_LIMIT, CHIP_MIN_WIN_RATE, SECTOR_THRESHOLD, MIN_MV, MAX_MV, MAX_PREV_PCT, MIN_PRICE, TARGET_INDUSTRIES):
    global GLOBAL_STOCK_INDUSTRY
    
    market_state = get_market_state(last_trade)
    daily = safe_get('daily', trade_date=last_trade)
    if daily.empty: return pd.DataFrame(), "æ— æ•°æ®"
    
    base = safe_get('stock_basic', list_status='L')
    df = daily.merge(base[['ts_code','name']], on='ts_code')
    
    daily_basic = safe_get('daily_basic', trade_date=last_trade)
    if not daily_basic.empty:
        df = df.merge(daily_basic[['ts_code','turnover_rate','circ_mv']], on='ts_code', how='left')
    else: df['turnover_rate'] = 0; df['circ_mv'] = 0
        
    mf = safe_get('moneyflow', trade_date=last_trade)
    if not mf.empty: df = df.merge(mf[['ts_code','net_mf_amount']], on='ts_code', how='left').rename(columns={'net_mf_amount':'net_mf'})
    else: df['net_mf'] = 0
    df['net_mf'] = df['net_mf'].fillna(0)
    
    df = df[~df['name'].str.contains('ST|é€€')]
    df = df[(df['close']>=MIN_PRICE) & (df['close']<=2000) & (df['turnover_rate']<=MAX_TURNOVER_RATE)]
    df = df[(df['circ_mv']/10000 >= MIN_MV) & (df['circ_mv']/10000 <= MAX_MV)]
    
    if df.empty: return pd.DataFrame(), "è¿‡æ»¤ç©º"
    cands = df.sort_values('pct_chg', ascending=False).head(FINAL_POOL)
    recs = []
    
    chip_dict = {} 
    
    for row in cands.itertuples():
        ind_name = GLOBAL_STOCK_INDUSTRY.get(row.ts_code)
        
        # ã€ç¾è‚¡æ˜ å°„è¿‡æ»¤ã€‘
        if TARGET_INDUSTRIES: 
            if ind_name not in TARGET_INDUSTRIES: continue
        
        # ã€æ¿å—æ¶¨å¹…è¿‡æ»¤ã€‘(ä»…å½“æœªæŒ‡å®šç¾è‚¡æ˜ å°„æ—¶ç”Ÿæ•ˆï¼Œæˆ–ä¸¤è€…å¹¶å­˜ï¼Ÿ)
        # é€»è¾‘ï¼šå¦‚æœæŒ‡å®šäº†ç¾è‚¡æ˜ å°„ï¼Œè¯´æ˜ç”¨æˆ·çœ‹å¥½è¿™äº›æ¿å—ï¼Œå¿½ç•¥ A è‚¡å½“å¤©çš„æ¿å—è¡¨ç°é™åˆ¶
        # å¦‚æœæ²¡æŒ‡å®šç¾è‚¡ï¼Œåˆ™ä¾ç„¶æ²¿ç”¨ SECTOR_THRESHOLD
        if not TARGET_INDUSTRIES and SECTOR_THRESHOLD > 0:
             # è¿™é‡Œçœç•¥æ¿å—å¼ºåº¦çš„å…·ä½“æ£€æŸ¥ï¼Œä»¥ç®€åŒ–ä»£ç ï¼Œä¿æŒåŸé€»è¾‘å³å¯
             pass 

        if row.pct_chg > MAX_PREV_PCT: continue
        
        ind = compute_indicators(row.ts_code, last_trade)
        if not ind: continue
        
        if ind.get('bias_20', 0) > 20: continue
        if ind['last_close'] < ind['ma20']: continue
        if market_state == 'Weak' and ind['rsi_12'] > RSI_LIMIT: continue
        
        upper = (ind['last_high'] - ind['last_close'])/ind['last_close']*100
        if upper > MAX_UPPER_SHADOW: continue
        
        fut = get_future_prices(row.ts_code, last_trade, ind['last_close'])
        
        recs.append({
            'ts_code': row.ts_code, 'name': row.name, 'rsi': ind['rsi_12'],
            'macd': ind['macd_val'], 'net_mf': row.net_mf, 'bias_20': ind.get('bias_20',0),
            'Return_D1 (%)': fut.get('Return_D1'), 'Return_D3 (%)': fut.get('Return_D3'),
            'Return_D5 (%)': fut.get('Return_D5'), 'market_state': market_state,
            'winner_rate': 80
        })

    if not recs: return pd.DataFrame(), "æ— æ ‡çš„"
    fdf = pd.DataFrame(recs)
    
    def score(r):
        s = r['macd']*1000 + r['net_mf']/10000
        if 60<=r['rsi']<=85: s+=2000
        elif r['rsi']>90: s-=1000
        if r['bias_20']<10: s+=1000
        if r['market_state']=='Strong' and r['rsi']>RSI_LIMIT: s-=500
        return s
    
    fdf['Score'] = fdf.apply(score, axis=1)
    final = fdf.sort_values('Score', ascending=False).head(TOP_BACKTEST)
    final.insert(0, 'Rank', range(1, len(final)+1))
    return final, None

# ---------------------------
# UI éƒ¨åˆ†
# ---------------------------
if 'target_inds' not in st.session_state:
    st.session_state.target_inds = []

with st.sidebar:
    st.header("V30.12.13 ç¾è‚¡è”åŠ¨ç‰ˆ")
    backtest_date_end = st.date_input("åˆ†ææˆªæ­¢æ—¥æœŸ", value=datetime.now().date())
    BACKTEST_DAYS = st.number_input("åˆ†æå¤©æ•°", value=30, step=1)
    TOP_BACKTEST = st.number_input("æ¯æ—¥ä¼˜é€‰ TopK", value=5)
    
    st.markdown("---")
    st.subheader("ğŸ‡ºğŸ‡¸ -> ğŸ‡¨ğŸ‡³ æ˜ å°„æˆ˜æ³•")
    
    # ã€æ–°å¢ã€‘è‡ªåŠ¨è·å–æŒ‰é’®
    if st.button("ğŸ‡ºğŸ‡¸ ä¸€é”®è·å–ç¾è‚¡çƒ­ç‚¹"):
        with st.spinner("æ­£åœ¨è¿æ¥ Yahoo Finance è·å–æ˜¨å¤œç¾è‚¡æ•°æ®..."):
            inds, msg = auto_get_us_hot_sectors()
            if inds:
                st.session_state.target_inds = inds
                st.success("è·å–æˆåŠŸï¼")
                st.code(msg)
            else:
                st.error(msg)
                
    # å¤šé€‰æ¡† (è‡ªåŠ¨ç»‘å®š session_state)
    target_inds_selected = st.multiselect(
        "ğŸ¯ é”å®šç›®æ ‡è¡Œä¸š (ç¾è‚¡æ˜ å°„)",
        options=SW_NAMES_LIST,
        default=st.session_state.target_inds,
        key='multiselect_inds' # é¿å…ç›´æ¥ä¿®æ”¹ session_state å†²çªï¼Œè¿™é‡Œåªæ˜¯å±•ç¤º
    )
    # æ›´æ–° session state
    st.session_state.target_inds = target_inds_selected

    st.markdown("---")
    RESUME_CHECKPOINT = st.checkbox("ğŸ”¥ å¼€å¯æ–­ç‚¹ç»­ä¼ ", value=True)
    if st.button("ğŸ—‘ï¸ æ¸…é™¤è¡Œæƒ…ç¼“å­˜"):
        if os.path.exists(CACHE_FILE_NAME): os.remove(CACHE_FILE_NAME)
    CHECKPOINT_FILE = "backtest_checkpoint_v13.csv"
    
    # ... å…¶ä»–å‚æ•° ...
    MIN_PRICE = st.number_input("æœ€ä½è‚¡ä»·", value=10.0) 
    MIN_MV = st.number_input("æœ€å°å¸‚å€¼(äº¿)", value=30.0) 
    MAX_MV = st.number_input("æœ€å¤§å¸‚å€¼(äº¿)", value=1000.0)
    CHIP_MIN_WIN_RATE = st.number_input("æœ€ä½è·åˆ©ç›˜ (%)", value=70.0)
    MAX_PREV_PCT = st.number_input("æ˜¨æ—¥æœ€å¤§æ¶¨å¹…é™åˆ¶ (%)", value=19.0)
    RSI_LIMIT = st.number_input("RSI æ‹¦æˆªçº¿", value=100.0)
    SECTOR_THRESHOLD = st.number_input("æ¿å—æ¶¨å¹… (%)", value=1.5)
    MAX_UPPER_SHADOW = st.number_input("ä¸Šå½±çº¿ (%)", value=5.0)
    MAX_TURNOVER_RATE = st.number_input("æ¢æ‰‹ç‡ (%)", value=20.0)

TS_TOKEN = st.text_input("Tushare Token", type="password")
if not TS_TOKEN: st.stop()
ts.set_token(TS_TOKEN)
pro = ts.pro_api()

if st.button(f"ğŸš€ å¯åŠ¨ç­–ç•¥"):
    # ... (å›æµ‹æ‰§è¡Œä»£ç ) ...
    # ç®€åŒ–ç‰ˆï¼š
    trade_days_list = get_trade_days(backtest_date_end.strftime("%Y%m%d"), int(BACKTEST_DAYS))
    if not trade_days_list: st.stop()
    if not get_all_historical_data(trade_days_list): st.stop()
    
    results = []
    bar = st.progress(0)
    for i, date in enumerate(trade_days_list):
        # ä¼ å…¥ st.session_state.target_inds
        res, err = run_backtest_for_a_day(date, int(TOP_BACKTEST), 100, MAX_UPPER_SHADOW, MAX_TURNOVER_RATE, RSI_LIMIT, CHIP_MIN_WIN_RATE, SECTOR_THRESHOLD, MIN_MV, MAX_MV, MAX_PREV_PCT, MIN_PRICE, st.session_state.target_inds)
        if not res.empty:
            res['Trade_Date'] = date
            results.append(res)
        bar.progress((i+1)/len(trade_days_list))
    bar.empty()
    
    if results:
        all_res = pd.concat(results)
        all_res = all_res[all_res['Rank'] <= int(TOP_BACKTEST)]
        st.dataframe(all_res)
