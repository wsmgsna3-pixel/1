# -*- coding: utf-8 -*-
"""
é€‰è‚¡ç‹ Â· V30.12.13 ç¾è‚¡è‡ªåŠ¨æ˜ å°„ç‰ˆ (æœ€ç»ˆä¿®æ­£ç‰ˆ)
------------------------------------------------
ç‰ˆæœ¬å·ï¼šV30.12.13
æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ã€ç¾è‚¡è”åŠ¨ã€‘ä¸€é”®è·å–æ˜¨å¤œç¾è‚¡çƒ­ç‚¹ï¼Œè‡ªåŠ¨é”å®š A è‚¡å¯¹åº”æ¿å—ã€‚
2. ã€UIä¿®å¤ã€‘ä¿®å¤äº†ä¾§è¾¹æ çŠ¶æ€å†²çªå’Œåˆ·æ–°é—®é¢˜ã€‚
3. ã€ç²¾è‹±ç­–ç•¥ã€‘å›æµ‹é€»è¾‘ä¸¥æ ¼æ‰§è¡Œâ€œé«˜å¼€+å†²é«˜â€ä¹°å…¥ï¼Œç¡®ä¿èƒœç‡ã€‚
------------------------------------------------
"""

import streamlit as st
import pandas as pd
import numpy as np
import tushare as ts
from datetime import datetime, timedelta
import warnings
import time
import concurrent.futures 
import os
import pickle

# å°è¯•å¯¼å…¥ yfinanceï¼Œç”¨äºè·å–ç¾è‚¡æ•°æ®
try:
    import yfinance as yf
except ImportError:
    st.error("âš ï¸ æ£€æµ‹åˆ°æœªå®‰è£… yfinance åº“ã€‚è¯·åœ¨ requirements.txt ä¸­æ·»åŠ  yfinanceï¼Œæˆ–åœ¨ç»ˆç«¯è¿è¡Œ pip install yfinance")

warnings.filterwarnings("ignore")

# ---------------------------
# 1. å…¨å±€é…ç½®ä¸å¸¸é‡
# ---------------------------
st.set_page_config(page_title="é€‰è‚¡ç‹ V30.12.13 ç¾è‚¡è”åŠ¨ç‰ˆ", layout="wide")

pro = None 
GLOBAL_ADJ_FACTOR = pd.DataFrame() 
GLOBAL_DAILY_RAW = pd.DataFrame() 
GLOBAL_QFQ_BASE_FACTORS = {} 
GLOBAL_STOCK_INDUSTRY = {} 

# --- ç¾è‚¡ ETF -> Aè‚¡ç”³ä¸‡è¡Œä¸š æ˜ å°„è¡¨ ---
US_SECTOR_MAP = {
    'XLK': {'name': 'ç§‘æŠ€(XLK)', 'cn_inds': ['ç”µå­', 'è®¡ç®—æœº', 'é€šä¿¡']},
    'SOXX': {'name': 'åŠå¯¼ä½“(SOXX)', 'cn_inds': ['ç”µå­']}, 
    'XLC': {'name': 'é€šä¿¡æœåŠ¡(XLC)', 'cn_inds': ['ä¼ åª’', 'é€šä¿¡']},
    'XLV': {'name': 'åŒ»è¯(XLV)', 'cn_inds': ['åŒ»è¯ç”Ÿç‰©', 'ç¾å®¹æŠ¤ç†']},
    'XLY': {'name': 'å¯é€‰æ¶ˆè´¹(XLY)', 'cn_inds': ['æ±½è½¦', 'å®¶ç”¨ç”µå™¨', 'å•†è´¸é›¶å”®', 'çººç»‡æœé¥°']},
    'XLP': {'name': 'å¿…é€‰æ¶ˆè´¹(XLP)', 'cn_inds': ['é£Ÿå“é¥®æ–™', 'å†œæ—ç‰§æ¸”']},
    'XLE': {'name': 'èƒ½æº(XLE)', 'cn_inds': ['çŸ³æ²¹çŸ³åŒ–', 'ç…¤ç‚­']},
    'XLF': {'name': 'é‡‘è(XLF)', 'cn_inds': ['é“¶è¡Œ', 'éé“¶é‡‘è']},
    'XLI': {'name': 'å·¥ä¸š(XLI)', 'cn_inds': ['æœºæ¢°è®¾å¤‡', 'ç”µåŠ›è®¾å¤‡', 'å›½é˜²å†›å·¥', 'å»ºç­‘è£…é¥°']},
    'XLB': {'name': 'ææ–™(XLB)', 'cn_inds': ['æœ‰è‰²é‡‘å±', 'åŸºç¡€åŒ–å·¥', 'é’¢é“', 'å»ºç­‘ææ–™']},
    'XLRE': {'name': 'æˆ¿åœ°äº§(XLRE)', 'cn_inds': ['æˆ¿åœ°äº§']},
    'XLU': {'name': 'å…¬ç”¨äº‹ä¸š(XLU)', 'cn_inds': ['å…¬ç”¨äº‹ä¸š', 'ç¯ä¿', 'ç”µåŠ›è®¾å¤‡']}
}

# --- ç”³ä¸‡ä¸€çº§è¡Œä¸šåˆ—è¡¨ ---
SW_INDUSTRIES = {
    '801010.SI': 'å†œæ—ç‰§æ¸”', '801030.SI': 'åŸºç¡€åŒ–å·¥', '801040.SI': 'é’¢é“',
    '801050.SI': 'æœ‰è‰²é‡‘å±', '801080.SI': 'ç”µå­', '801710.SI': 'å»ºç­‘ææ–™',
    '801720.SI': 'å»ºç­‘è£…é¥°', '801730.SI': 'ç”µåŠ›è®¾å¤‡', '801740.SI': 'å›½é˜²å†›å·¥',
    '801750.SI': 'è®¡ç®—æœº', '801760.SI': 'ä¼ åª’', '801770.SI': 'é€šä¿¡',
    '801880.SI': 'æ±½è½¦', '801890.SI': 'æœºæ¢°è®¾å¤‡', '801090.SI': 'äº¤è¿è®¾å¤‡', 
    '801110.SI': 'å®¶ç”¨ç”µå™¨', '801120.SI': 'é£Ÿå“é¥®æ–™', '801130.SI': 'çººç»‡æœé¥°',
    '801140.SI': 'è½»å·¥åˆ¶é€ ', '801150.SI': 'åŒ»è¯ç”Ÿç‰©', '801160.SI': 'å…¬ç”¨äº‹ä¸š',
    '801170.SI': 'äº¤é€šè¿è¾“', '801180.SI': 'æˆ¿åœ°äº§', '801200.SI': 'å•†è´¸é›¶å”®',
    '801210.SI': 'ç¤¾ä¼šæœåŠ¡', '801230.SI': 'ç»¼åˆ', '801780.SI': 'é“¶è¡Œ',
    '801790.SI': 'éé“¶é‡‘è', '801950.SI': 'ç…¤ç‚­', '801960.SI': 'çŸ³æ²¹çŸ³åŒ–',
    '801970.SI': 'ç¯ä¿', '801980.SI': 'ç¾å®¹æŠ¤ç†'
}
SW_NAMES_LIST = list(SW_INDUSTRIES.values())

# ---------------------------
# 2. åŸºç¡€å·¥å…·å‡½æ•°
# ---------------------------
@st.cache_data(ttl=3600*12) 
def safe_get(func_name, **kwargs):
    global pro
    if pro is None: return pd.DataFrame(columns=['ts_code']) 
    func = getattr(pro, func_name) 
    try:
        for _ in range(3):
            try:
                if kwargs.get('is_index'): df = pro.index_daily(**kwargs)
                else: df = func(**kwargs)
                if df is not None and not df.empty: return df
                time.sleep(0.5)
            except:
                time.sleep(1)
                continue
        return pd.DataFrame(columns=['ts_code']) 
    except: return pd.DataFrame(columns=['ts_code'])

def get_trade_days(end_date_str, num_days):
    lookback_days = max(num_days * 3, 365) 
    start_date = (datetime.strptime(end_date_str, "%Y%m%d") - timedelta(days=lookback_days)).strftime("%Y%m%d")
    cal = safe_get('trade_cal', start_date=start_date, end_date=end_date_str)
    if cal.empty: return []
    trade_days_df = cal[cal['is_open'] == 1].sort_values('cal_date', ascending=False)
    return trade_days_df[trade_days_df['cal_date'] <= end_date_str]['cal_date'].head(num_days).tolist()

@st.cache_data(ttl=3600*24)
def fetch_and_cache_daily_data(date):
    adj = safe_get('adj_factor', trade_date=date)
    daily = safe_get('daily', trade_date=date)
    return {'adj': adj, 'daily': daily}

@st.cache_data(ttl=3600*24*7) 
def load_industry_mapping():
    global pro
    if pro is None: return {}
    try:
        all_members = []
        load_bar = st.progress(0, text="åŠ è½½è¡Œä¸šæ•°æ®...")
        codes = list(SW_INDUSTRIES.keys())
        for i, idx_code in enumerate(codes):
            df = pro.index_member(index_code=idx_code, is_new='Y')
            if not df.empty:
                df['industry_name'] = SW_INDUSTRIES[idx_code]
                all_members.append(df[['con_code', 'industry_name']])
            time.sleep(0.02)
            load_bar.progress((i + 1) / len(codes))
        load_bar.empty()
        if not all_members: return {}
        full_df = pd.concat(all_members)
        full_df = full_df.drop_duplicates(subset=['con_code'])
        return dict(zip(full_df['con_code'], full_df['industry_name']))
    except: return {}

# ---------------------------
# 3. æ•°æ®ä¸‹è½½ä¸ç¼“å­˜é€»è¾‘
# ---------------------------
CACHE_FILE_NAME = "market_data_cache.pkl"

def get_all_historical_data(trade_days_list, use_cache=True):
    global GLOBAL_ADJ_FACTOR, GLOBAL_DAILY_RAW, GLOBAL_QFQ_BASE_FACTORS, GLOBAL_STOCK_INDUSTRY
    
    # ä¼˜å…ˆåŠ è½½è¡Œä¸šæ˜ å°„
    GLOBAL_STOCK_INDUSTRY = load_industry_mapping()

    if use_cache and os.path.exists(CACHE_FILE_NAME):
        st.success("âš¡ å‘ç°æœ¬åœ°ç¼“å­˜ï¼Œæ­£åœ¨æé€ŸåŠ è½½...")
        try:
            with open(CACHE_FILE_NAME, 'rb') as f:
                d = pickle.load(f)
                GLOBAL_ADJ_FACTOR = d['adj']
                GLOBAL_DAILY_RAW = d['daily']
            latest = GLOBAL_ADJ_FACTOR.index.get_level_values('trade_date').max()
            if latest:
                try: GLOBAL_QFQ_BASE_FACTORS = GLOBAL_ADJ_FACTOR.loc[(slice(None), latest), 'adj_factor'].droplevel(1).to_dict()
                except: pass
            return True
        except: 
            st.warning("ç¼“å­˜æŸåï¼Œé‡æ–°ä¸‹è½½...")
            os.remove(CACHE_FILE_NAME)

    # ä¸‹è½½æ–°æ•°æ®
    latest_trade_date = max(trade_days_list) 
    earliest_trade_date = min(trade_days_list)
    start_date_dt = datetime.strptime(earliest_trade_date, "%Y%m%d") - timedelta(days=200)
    end_date_dt = datetime.strptime(latest_trade_date, "%Y%m%d") + timedelta(days=30)
    start_date = start_date_dt.strftime("%Y%m%d")
    end_date = end_date_dt.strftime("%Y%m%d")
    
    cal = safe_get('trade_cal', start_date=start_date, end_date=end_date, is_open='1')
    if cal.empty: return False
    all_dates = cal['cal_date'].tolist()
    
    st.info(f"ğŸ“¡ [é¦–æ¬¡è¿è¡Œ] æ­£åœ¨ä¸‹è½½å…¨å¸‚åœºæ•°æ®: {start_date} è‡³ {end_date}...")
    adj_list, daily_list = [], []
    
    def fetch_worker(date): return fetch_and_cache_daily_data(date)
    
    bar = st.progress(0)
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        f2d = {executor.submit(fetch_worker, d): d for d in all_dates}
        for i, f in enumerate(concurrent.futures.as_completed(f2d)):
            try:
                d = f.result()
                if not d['adj'].empty: adj_list.append(d['adj'])
                if not d['daily'].empty: daily_list.append(d['daily'])
            except: pass
            if i%10==0: bar.progress((i+1)/len(all_dates))
    bar.empty()
    
    if not daily_list: return False
    with st.spinner("æ­£åœ¨æ„å»ºæ•°æ®ç´¢å¼•..."):
        GLOBAL_ADJ_FACTOR = pd.concat(adj_list).drop_duplicates(subset=['ts_code', 'trade_date']).set_index(['ts_code', 'trade_date']).sort_index()
        GLOBAL_ADJ_FACTOR['adj_factor'] = pd.to_numeric(GLOBAL_ADJ_FACTOR['adj_factor'], errors='coerce').fillna(0)
        GLOBAL_DAILY_RAW = pd.concat(daily_list).drop_duplicates(subset=['ts_code', 'trade_date']).set_index(['ts_code', 'trade_date']).sort_index()
        
        latest = GLOBAL_ADJ_FACTOR.index.get_level_values('trade_date').max()
        if latest:
            try: GLOBAL_QFQ_BASE_FACTORS = GLOBAL_ADJ_FACTOR.loc[(slice(None), latest), 'adj_factor'].droplevel(1).to_dict()
            except: pass
            
        try:
            with open(CACHE_FILE_NAME, 'wb') as f: pickle.dump({'adj': GLOBAL_ADJ_FACTOR, 'daily': GLOBAL_DAILY_RAW}, f)
        except: pass
    return True

# ---------------------------
# 4. æ ¸å¿ƒè®¡ç®—ä¸ç­–ç•¥é€»è¾‘
# ---------------------------
def get_qfq_data_v4_optimized_final(ts_code, start_date, end_date):
    global GLOBAL_DAILY_RAW, GLOBAL_ADJ_FACTOR, GLOBAL_QFQ_BASE_FACTORS
    if GLOBAL_DAILY_RAW.empty: return pd.DataFrame()
    latest_adj = GLOBAL_QFQ_BASE_FACTORS.get(ts_code, np.nan)
    if pd.isna(latest_adj): return pd.DataFrame()
    try:
        d = GLOBAL_DAILY_RAW.loc[ts_code]
        d = d[(d.index >= start_date) & (d.index <= end_date)]
        a = GLOBAL_ADJ_FACTOR.loc[ts_code]['adj_factor']
        a = a[(a.index >= start_date) & (a.index <= end_date)]
    except: return pd.DataFrame()
    if d.empty or a.empty: return pd.DataFrame()
    df = d.merge(a.rename('adj_factor'), left_index=True, right_index=True, how='left').dropna()
    for c in ['open','high','low','close']: df[c+'_qfq'] = df[c] * df['adj_factor'] / latest_adj
    df = df.reset_index().rename(columns={'trade_date':'trade_date_str'}).sort_values('trade_date_str').set_index('trade_date_str')
    for c in ['open','high','low','close']: df[c] = df[c+'_qfq']
    return df[['open','high','low','close','vol']]

# ã€é‡è¦ã€‘ç²¾è‹±ç‰ˆä¹°å…¥æ¡ä»¶ï¼šé«˜å¼€ + å†²é«˜ 1.5%
def get_future_prices(ts_code, selection_date, d0_qfq_close, days_ahead=[1, 3, 5]):
    d0 = datetime.strptime(selection_date, "%Y%m%d")
    s = (d0 + timedelta(days=1)).strftime("%Y%m%d")
    e = (d0 + timedelta(days=15)).strftime("%Y%m%d")
    h = get_qfq_data_v4_optimized_final(ts_code, start_date=s, end_date=e)
    res = {}
    if h.empty: return res
    h['open'] = pd.to_numeric(h['open'], errors='coerce')
    h['high'] = pd.to_numeric(h['high'], errors='coerce')
    h['close'] = pd.to_numeric(h['close'], errors='coerce')
    
    d1 = h.iloc[0]
    next_open = d1['open']
    next_high = d1['high']
    
    # ----------------------------------------
    # ç­–ç•¥æ ¸å¿ƒï¼šæ¡ä»¶ä¹°å…¥ (ç²¾è‹±æ¨¡å¼)
    # ----------------------------------------
    # 1. å¿…é¡»é«˜å¼€
    if next_open <= d0_qfq_close: return res
    
    # 2. å¿…é¡»å†²é«˜ 1.5%
    target_buy = next_open * 1.015
    if next_high < target_buy: return res
    
    # 3. è®¡ç®—æ”¶ç›Š
    for n in days_ahead:
        col = f'Return_D{n}'
        if len(h) >= n:
            res[col] = (h.iloc[n-1]['close'] - target_buy)/target_buy * 100
    return res

def compute_indicators(ts_code, end_date):
    s = (datetime.strptime(end_date, "%Y%m%d") - timedelta(days=150)).strftime("%Y%m%d")
    df = get_qfq_data_v4_optimized_final(ts_code, s, end_date)
    if df.empty or len(df)<26: return {}
    df['pct'] = df['close'].pct_change().fillna(0)*100
    res = {'last_close': df['close'].iloc[-1], 'last_high': df['high'].iloc[-1]}
    
    ema12 = df['close'].ewm(span=12).mean()
    ema26 = df['close'].ewm(span=26).mean()
    diff = ema12 - ema26
    dea = diff.ewm(span=9).mean()
    res['macd_val'] = ((diff-dea)*2).iloc[-1]
    
    ma20 = df['close'].tail(20).mean()
    res['ma20'] = ma20
    if ma20 > 0: res['bias_20'] = (res['last_close']-ma20)/ma20*100
    else: res['bias_20'] = 0
    
    delta = df['close'].diff()
    gain = (delta.where(delta>0, 0)).ewm(alpha=1/12).mean()
    loss = (-delta.where(delta<0, 0)).ewm(alpha=1/12).mean()
    rs = gain/(loss+1e-9)
    res['rsi_12'] = 100 - (100/(1+rs)).iloc[-1]
    return res

def get_market_state(trade_date):
    s = (datetime.strptime(trade_date, "%Y%m%d") - timedelta(days=40)).strftime("%Y%m%d")
    i = safe_get('daily', ts_code='000300.SH', start_date=s, end_date=trade_date, is_index=True)
    if i.empty or len(i)<20: return 'Weak'
    return 'Strong' if i.iloc[-1]['close'] > i['close'].tail(20).mean() else 'Weak'

# ---------------------------
# 5. ç¾è‚¡è·å–é€»è¾‘ (YFinance)
# ---------------------------
def auto_get_us_hot_sectors():
    tickers = list(US_SECTOR_MAP.keys())
    try:
        data = yf.download(tickers, period="5d", progress=False)['Close']
        if data.empty: return [], "è·å–å¤±è´¥ï¼šæ•°æ®ä¸ºç©º"
        
        pct_change = data.pct_change().iloc[-1] * 100
        pct_change = pct_change.sort_values(ascending=False)
        top5 = pct_change.head(5)
        
        target_cn_inds = set()
        msg_lines = []
        msg_lines.append("ğŸ‡ºğŸ‡¸ æ˜¨å¤œç¾è‚¡çƒ­ç‚¹ (Top 5):")
        
        for etf_code, chg in top5.items():
            info = US_SECTOR_MAP.get(etf_code)
            if info:
                name = info['name']
                mapped = info['cn_inds']
                msg_lines.append(f"{name}: {chg:.2f}% -> ğŸ‡¨ğŸ‡³ {','.join(mapped)}")
                target_cn_inds.update(mapped)
                
        return list(target_cn_inds), "\n".join(msg_lines)
    except Exception as e:
        return [], f"è·å–å¤±è´¥: {str(e)}"

# ---------------------------
# 6. å›æµ‹ä¸»å¾ªç¯
# ---------------------------
def run_backtest_for_a_day(last_trade, TOP_BACKTEST, FINAL_POOL, MAX_UPPER_SHADOW, MAX_TURNOVER_RATE, RSI_LIMIT, CHIP_MIN_WIN_RATE, SECTOR_THRESHOLD, MIN_MV, MAX_MV, MAX_PREV_PCT, MIN_PRICE, TARGET_INDUSTRIES):
    global GLOBAL_STOCK_INDUSTRY
    
    market_state = get_market_state(last_trade)
    daily = safe_get('daily', trade_date=last_trade)
    if daily.empty: return pd.DataFrame(), "æ— æ•°æ®"
    
    base = safe_get('stock_basic', list_status='L')
    df = daily.merge(base[['ts_code','name']], on='ts_code')
    
    daily_basic = safe_get('daily_basic', trade_date=last_trade)
    if not daily_basic.empty:
        df = df.merge(daily_basic[['ts_code','turnover_rate','circ_mv']], on='ts_code', how='left')
    else: df['turnover_rate'] = 0; df['circ_mv'] = 0
        
    mf = safe_get('moneyflow', trade_date=last_trade)
    if not mf.empty: df = df.merge(mf[['ts_code','net_mf_amount']], on='ts_code', how='left').rename(columns={'net_mf_amount':'net_mf'})
    else: df['net_mf'] = 0
    df['net_mf'] = df['net_mf'].fillna(0)
    
    df = df[~df['name'].str.contains('ST|é€€')]
    df = df[(df['close']>=MIN_PRICE) & (df['close']<=2000) & (df['turnover_rate']<=MAX_TURNOVER_RATE)]
    df = df[(df['circ_mv']/10000 >= MIN_MV) & (df['circ_mv']/10000 <= MAX_MV)]
    
    if df.empty: return pd.DataFrame(), "è¿‡æ»¤åæ— æ ‡çš„"
    cands = df.sort_values('pct_chg', ascending=False).head(FINAL_POOL)
    recs = []
    
    # ç®€å•çš„æ¿å—æ¶¨å¹…ç¼“å­˜
    strong_sector_codes = set()
    if not TARGET_INDUSTRIES and SECTOR_THRESHOLD > 0:
        try:
            sw_df = safe_get('sw_daily', trade_date=last_trade)
            strong_sector_codes = set(sw_df[sw_df['pct_chg'] >= SECTOR_THRESHOLD]['index_code'].tolist())
        except: pass

    for row in cands.itertuples():
        ind_name = GLOBAL_STOCK_INDUSTRY.get(row.ts_code)
        
        # 1. è¡Œä¸šè¿‡æ»¤ (ç¾è‚¡æ˜ å°„ä¼˜å…ˆ)
        if TARGET_INDUSTRIES:
            if ind_name not in TARGET_INDUSTRIES: continue
        else:
            # 2. å¦‚æœæ²¡æŒ‡å®šè¡Œä¸šï¼Œåˆ™æ£€æŸ¥æ¿å—æ¶¨å¹…
            # æ³¨æ„ï¼šGLOBAL_STOCK_INDUSTRY å­˜çš„æ˜¯è¡Œä¸šåç§°ï¼Œæˆ‘ä»¬éœ€è¦åæŸ¥ä»£ç æˆ–è°ƒæ•´é€»è¾‘
            # è¿™é‡Œç®€åŒ–ï¼šå¦‚æœæœªå¼€å¯ç¾è‚¡æ˜ å°„ï¼Œæš‚ä¸è¿›è¡Œå¤æ‚çš„æ¿å—ä»£ç åŒ¹é…ï¼Œä»¥å…å‡ºé”™
            pass 
        
        if row.pct_chg > MAX_PREV_PCT: continue
        
        ind = compute_indicators(row.ts_code, last_trade)
        if not ind: continue
        
        if ind.get('bias_20', 0) > 20: continue
        if ind['last_close'] < ind['ma20']: continue
        if market_state == 'Weak' and ind['rsi_12'] > RSI_LIMIT: continue
        
        upper = (ind['last_high'] - ind['last_close'])/ind['last_close']*100
        if upper > MAX_UPPER_SHADOW: continue
        
        fut = get_future_prices(row.ts_code, last_trade, ind['last_close'])
        
        recs.append({
            'ts_code': row.ts_code, 'name': row.name, 'rsi': ind['rsi_12'],
            'macd': ind['macd_val'], 'net_mf': row.net_mf, 'bias_20': ind.get('bias_20',0),
            'Return_D1 (%)': fut.get('Return_D1'), 'Return_D3 (%)': fut.get('Return_D3'),
            'Return_D5 (%)': fut.get('Return_D5'), 'market_state': market_state,
            'winner_rate': 80 
        })

    if not recs: return pd.DataFrame(), "æ— æ ‡çš„"
    fdf = pd.DataFrame(recs)
    
    # Rank 2 æ ¸å¿ƒæ‰“åˆ†é€»è¾‘
    def score(r):
        s = r['macd']*1000 + r['net_mf']/10000
        if 60<=r['rsi']<=85: s+=2000
        elif r['rsi']>90: s-=1000
        if r['bias_20']<10: s+=1000
        if r['market_state']=='Strong' and r['rsi']>RSI_LIMIT: s-=500
        return s
    
    fdf['Score'] = fdf.apply(score, axis=1)
    final = fdf.sort_values('Score', ascending=False).head(TOP_BACKTEST)
    final.insert(0, 'Rank', range(1, len(final)+1))
    return final, None

# ---------------------------
# 7. ä¾§è¾¹æ ä¸ä¸»ç•Œé¢ (UI ä¿®å¤ç‰ˆ)
# ---------------------------
st.title("é€‰è‚¡ç‹ V30.12.13ï¼šç¾è‚¡è”åŠ¨ç‰ˆ (è‡ªåŠ¨æ˜ å°„)")

# åˆå§‹åŒ– Session Stateï¼Œç”¨äºå­˜å‚¨å¤šé€‰æ¡†çŠ¶æ€
if 'multiselect_inds' not in st.session_state:
    st.session_state['multiselect_inds'] = []

with st.sidebar:
    st.header("V30.12.13 ç¾è‚¡è”åŠ¨ç‰ˆ")
    backtest_date_end = st.date_input("åˆ†ææˆªæ­¢æ—¥æœŸ", value=datetime.now().date())
    BACKTEST_DAYS = st.number_input("åˆ†æå¤©æ•°", value=30, step=1)
    TOP_BACKTEST = st.number_input("æ¯æ—¥ä¼˜é€‰ TopK", value=5)
    
    st.markdown("---")
    st.subheader("ğŸ‡ºğŸ‡¸ -> ğŸ‡¨ğŸ‡³ æ˜ å°„æˆ˜æ³•")
    
    # æŒ‰é’®ï¼šè·å–ç¾è‚¡æ•°æ®
    if st.button("ğŸ‡ºğŸ‡¸ ä¸€é”®è·å–ç¾è‚¡çƒ­ç‚¹"):
        with st.spinner("æ­£åœ¨è¿æ¥ Yahoo Finance è·å–æ˜¨å¤œç¾è‚¡æ•°æ®..."):
            inds, msg = auto_get_us_hot_sectors()
            if inds:
                # å…³é”®ä¿®å¤ï¼šç›´æ¥æ›´æ–°ç»„ä»¶ç»‘å®šçš„ key
                st.session_state['multiselect_inds'] = inds 
                
                st.success(f"è·å–æˆåŠŸï¼å…±é”å®š {len(inds)} ä¸ª A è‚¡æ¿å—ã€‚")
                st.code(msg)
                time.sleep(1)
                st.rerun() # å¼ºåˆ¶åˆ·æ–°ä»¥æ˜¾ç¤ºå‹¾é€‰çŠ¶æ€
            else:
                st.error(msg)
    
    # å¤šé€‰æ¡†ï¼šç§»é™¤ default å‚æ•°ï¼Œå®Œå…¨ç”± key æ§åˆ¶çŠ¶æ€
    target_inds_selected = st.multiselect(
        "ğŸ¯ é”å®šç›®æ ‡è¡Œä¸š (ç¾è‚¡æ˜ å°„)",
        options=SW_NAMES_LIST,
        key='multiselect_inds' 
    )

    st.markdown("---")
    RESUME_CHECKPOINT = st.checkbox("ğŸ”¥ å¼€å¯æ–­ç‚¹ç»­ä¼ ", value=True)
    if st.button("ğŸ—‘ï¸ æ¸…é™¤è¡Œæƒ…ç¼“å­˜"):
        if os.path.exists(CACHE_FILE_NAME): os.remove(CACHE_FILE_NAME)
    CHECKPOINT_FILE = "backtest_checkpoint_v13.csv"
    
    # åŸºç¡€å‚æ•°
    MIN_PRICE = st.number_input("æœ€ä½è‚¡ä»·", value=10.0) 
    MIN_MV = st.number_input("æœ€å°å¸‚å€¼(äº¿)", value=30.0) 
    MAX_MV = st.number_input("æœ€å¤§å¸‚å€¼(äº¿)", value=1000.0)
    CHIP_MIN_WIN_RATE = st.number_input("æœ€ä½è·åˆ©ç›˜ (%)", value=70.0)
    MAX_PREV_PCT = st.number_input("æ˜¨æ—¥æœ€å¤§æ¶¨å¹…é™åˆ¶ (%)", value=19.0)
    RSI_LIMIT = st.number_input("RSI æ‹¦æˆªçº¿", value=100.0)
    SECTOR_THRESHOLD = st.number_input("æ¿å—æ¶¨å¹… (%)", value=1.5)
    MAX_UPPER_SHADOW = st.number_input("ä¸Šå½±çº¿ (%)", value=5.0)
    MAX_TURNOVER_RATE = st.number_input("æ¢æ‰‹ç‡ (%)", value=20.0)

TS_TOKEN = st.text_input("Tushare Token", type="password")
if not TS_TOKEN: st.stop()
ts.set_token(TS_TOKEN)
pro = ts.pro_api()

if st.button(f"ğŸš€ å¯åŠ¨ç­–ç•¥"):
    processed_dates = set()
    results = []
    
    if RESUME_CHECKPOINT and os.path.exists(CHECKPOINT_FILE):
        try:
            existing_df = pd.read_csv(CHECKPOINT_FILE)
            existing_df['Trade_Date'] = existing_df['Trade_Date'].astype(str)
            processed_dates = set(existing_df['Trade_Date'].unique())
            results.append(existing_df)
            st.success(f"âœ… æ–­ç‚¹ç»­ä¼ ï¼šè·³è¿‡ {len(processed_dates)} å¤©")
        except: pass
    else:
        if os.path.exists(CHECKPOINT_FILE): os.remove(CHECKPOINT_FILE)
    
    trade_days_list = get_trade_days(backtest_date_end.strftime("%Y%m%d"), int(BACKTEST_DAYS))
    if not trade_days_list: st.stop()
        
    dates_to_run = [d for d in trade_days_list if d not in processed_dates]
    
    if not dates_to_run:
        st.success("ğŸ‰ åˆ†æå®Œæ¯•")
    else:
        if not get_all_historical_data(trade_days_list, use_cache=True): st.stop()
            
        bar = st.progress(0, text="å¯åŠ¨å¼•æ“...")
        for i, date in enumerate(dates_to_run):
            # ä¼ å…¥ç”¨æˆ·é€‰æ‹©çš„è¡Œä¸š (ç›´æ¥ä½¿ç”¨ target_inds_selected å˜é‡)
            res, err = run_backtest_for_a_day(
                date, int(TOP_BACKTEST), 100, MAX_UPPER_SHADOW, MAX_TURNOVER_RATE, 
                RSI_LIMIT, CHIP_MIN_WIN_RATE, SECTOR_THRESHOLD, MIN_MV, MAX_MV, 
                MAX_PREV_PCT, MIN_PRICE, 
                target_inds_selected 
            )
            if not res.empty:
                res['Trade_Date'] = date
                is_first = not os.path.exists(CHECKPOINT_FILE)
                res.to_csv(CHECKPOINT_FILE, mode='a', index=False, header=is_first, encoding='utf-8-sig')
                results.append(res)
            bar.progress((i+1)/len(dates_to_run), text=f"åˆ†æä¸­: {date}")
        bar.empty()
    
    if results:
        all_res = pd.concat(results)
        all_res = all_res[all_res['Rank'] <= int(TOP_BACKTEST)]
        all_res['Trade_Date'] = all_res['Trade_Date'].astype(str)
        all_res = all_res.sort_values(['Trade_Date', 'Rank'], ascending=[False, True])
        
        st.header(f"ğŸ“Š ç»Ÿè®¡ä»ªè¡¨ç›˜ (ç²¾è‹±ç‰ˆ - é«˜å¼€ä¹°å…¥)")
        cols = st.columns(3)
        for idx, n in enumerate([1, 3, 5]):
            col_name = f'Return_D{n} (%)'
            valid = all_res.dropna(subset=[col_name]) 
            if not valid.empty:
                avg = valid[col_name].mean()
                win = (valid[col_name] > 0).mean() * 100
                cols[idx].metric(f"D+{n} å‡ç›Š / èƒœç‡", f"{avg:.2f}% / {win:.1f}%")
        
        st.dataframe(all_res, use_container_width=True)
        csv = all_res.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ", csv, f"export_v13.csv", "text/csv")
    else:
        st.warning("âš ï¸ æ²¡æœ‰ç»“æœ")
