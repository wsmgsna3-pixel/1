# -*- coding: utf-8 -*-
"""
é€‰è‚¡ç‹ Â· V30.12.15 çœŸå®å›æµ‹ä¿®å¤ç‰ˆ (Tushare/YFinance åŒæ¨¡)
------------------------------------------------
ç‰ˆæœ¬å·ï¼šV30.12.15
ä¿®å¤å†…å®¹ï¼š
1. ã€ä»£ç ä¿®æ­£ã€‘Tushare ç¾è‚¡ä»£ç å»æ‰ .US åç¼€ï¼ŒåŒ¹é…æ¥å£æ ‡å‡†ã€‚
2. ã€åŒé‡ä¿éšœã€‘ä¼˜å…ˆ Tushareï¼Œå¤±è´¥è‡ªåŠ¨åˆ‡ YFinanceï¼Œç¡®ä¿æ•°æ®å¿…è¾¾ã€‚
3. ã€æ•°æ®ç›‘æ§ã€‘ä¾§è¾¹æ æ˜¾ç¤ºç¾è‚¡æ•°æ®è·å–çŠ¶æ€ï¼Œæ‹’ç»ç›²è·‘ã€‚
------------------------------------------------
"""

import streamlit as st
import pandas as pd
import numpy as np
import tushare as ts
from datetime import datetime, timedelta
import warnings
import time
import concurrent.futures 
import os
import pickle

# å°è¯•å¯¼å…¥ yfinance ä½œä¸ºå¤‡ç”¨
try:
    import yfinance as yf
except ImportError:
    yf = None

warnings.filterwarnings("ignore")

# ---------------------------
# 1. å…¨å±€é…ç½®
# ---------------------------
st.set_page_config(page_title="é€‰è‚¡ç‹ V30.12.15 ä¿®å¤ç‰ˆ", layout="wide")

pro = None 
GLOBAL_ADJ_FACTOR = pd.DataFrame() 
GLOBAL_DAILY_RAW = pd.DataFrame() 
GLOBAL_QFQ_BASE_FACTORS = {} 
GLOBAL_STOCK_INDUSTRY = {} 
GLOBAL_US_DATA = pd.DataFrame() 

# --- ç¾è‚¡ ETF ä»£ç è¡¨ (å»æ‰ .US åç¼€ä»¥é€‚é… Tushare) ---
US_ETF_MAP = {
    'XLK': {'name': 'ç§‘æŠ€(XLK)', 'cn_inds': ['ç”µå­', 'è®¡ç®—æœº', 'é€šä¿¡']},
    'SOXX': {'name': 'åŠå¯¼ä½“(SOXX)', 'cn_inds': ['ç”µå­']}, 
    'XLC': {'name': 'é€šä¿¡(XLC)', 'cn_inds': ['ä¼ åª’', 'é€šä¿¡']},
    'XLV': {'name': 'åŒ»è¯(XLV)', 'cn_inds': ['åŒ»è¯ç”Ÿç‰©', 'ç¾å®¹æŠ¤ç†']},
    'XLY': {'name': 'å¯é€‰(XLY)', 'cn_inds': ['æ±½è½¦', 'å®¶ç”¨ç”µå™¨', 'å•†è´¸é›¶å”®', 'çººç»‡æœé¥°']},
    'XLP': {'name': 'å¿…é€‰(XLP)', 'cn_inds': ['é£Ÿå“é¥®æ–™', 'å†œæ—ç‰§æ¸”']},
    'XLE': {'name': 'èƒ½æº(XLE)', 'cn_inds': ['çŸ³æ²¹çŸ³åŒ–', 'ç…¤ç‚­']},
    'XLF': {'name': 'é‡‘è(XLF)', 'cn_inds': ['é“¶è¡Œ', 'éé“¶é‡‘è']},
    'XLI': {'name': 'å·¥ä¸š(XLI)', 'cn_inds': ['æœºæ¢°è®¾å¤‡', 'ç”µåŠ›è®¾å¤‡', 'å›½é˜²å†›å·¥', 'å»ºç­‘è£…é¥°']},
    'XLB': {'name': 'ææ–™(XLB)', 'cn_inds': ['æœ‰è‰²é‡‘å±', 'åŸºç¡€åŒ–å·¥', 'é’¢é“', 'å»ºç­‘ææ–™']},
    'XLRE': {'name': 'åœ°äº§(XLRE)', 'cn_inds': ['æˆ¿åœ°äº§']},
    'XLU': {'name': 'å…¬ç”¨(XLU)', 'cn_inds': ['å…¬ç”¨äº‹ä¸š', 'ç¯ä¿', 'ç”µåŠ›è®¾å¤‡']}
}
US_TICKERS = list(US_ETF_MAP.keys())

# --- ç”³ä¸‡ä¸€çº§è¡Œä¸šåˆ—è¡¨ ---
SW_INDUSTRIES = {
    '801010.SI': 'å†œæ—ç‰§æ¸”', '801030.SI': 'åŸºç¡€åŒ–å·¥', '801040.SI': 'é’¢é“',
    '801050.SI': 'æœ‰è‰²é‡‘å±', '801080.SI': 'ç”µå­', '801710.SI': 'å»ºç­‘ææ–™',
    '801720.SI': 'å»ºç­‘è£…é¥°', '801730.SI': 'ç”µåŠ›è®¾å¤‡', '801740.SI': 'å›½é˜²å†›å·¥',
    '801750.SI': 'è®¡ç®—æœº', '801760.SI': 'ä¼ åª’', '801770.SI': 'é€šä¿¡',
    '801880.SI': 'æ±½è½¦', '801890.SI': 'æœºæ¢°è®¾å¤‡', '801090.SI': 'äº¤è¿è®¾å¤‡', 
    '801110.SI': 'å®¶ç”¨ç”µå™¨', '801120.SI': 'é£Ÿå“é¥®æ–™', '801130.SI': 'çººç»‡æœé¥°',
    '801140.SI': 'è½»å·¥åˆ¶é€ ', '801150.SI': 'åŒ»è¯ç”Ÿç‰©', '801160.SI': 'å…¬ç”¨äº‹ä¸š',
    '801170.SI': 'äº¤é€šè¿è¾“', '801180.SI': 'æˆ¿åœ°äº§', '801200.SI': 'å•†è´¸é›¶å”®',
    '801210.SI': 'ç¤¾ä¼šæœåŠ¡', '801230.SI': 'ç»¼åˆ', '801780.SI': 'é“¶è¡Œ',
    '801790.SI': 'éé“¶é‡‘è', '801950.SI': 'ç…¤ç‚­', '801960.SI': 'çŸ³æ²¹çŸ³åŒ–',
    '801970.SI': 'ç¯ä¿', '801980.SI': 'ç¾å®¹æŠ¤ç†'
}

# ---------------------------
# 2. åŸºç¡€å‡½æ•°
# ---------------------------
@st.cache_data(ttl=3600*12) 
def safe_get(func_name, **kwargs):
    global pro
    if pro is None: return pd.DataFrame(columns=['ts_code']) 
    func = getattr(pro, func_name) 
    try:
        for _ in range(3):
            try:
                if kwargs.get('is_index'): 
                    kwargs.pop('is_index')
                    df = pro.index_daily(**kwargs)
                else: 
                    df = func(**kwargs)
                if df is not None and not df.empty: return df
                time.sleep(0.5)
            except:
                time.sleep(1)
                continue
        return pd.DataFrame(columns=['ts_code']) 
    except: return pd.DataFrame(columns=['ts_code'])

def get_trade_days(end_date_str, num_days):
    lookback_days = max(num_days * 3, 365) 
    start_date = (datetime.strptime(end_date_str, "%Y%m%d") - timedelta(days=lookback_days)).strftime("%Y%m%d")
    cal = safe_get('trade_cal', start_date=start_date, end_date=end_date_str)
    if cal.empty: return []
    trade_days_df = cal[cal['is_open'] == 1].sort_values('cal_date', ascending=False)
    return trade_days_df[trade_days_df['cal_date'] <= end_date_str]['cal_date'].head(num_days).tolist()

@st.cache_data(ttl=3600*24)
def fetch_and_cache_daily_data(date):
    adj = safe_get('adj_factor', trade_date=date)
    daily = safe_get('daily', trade_date=date)
    return {'adj': adj, 'daily': daily}

@st.cache_data(ttl=3600*24*7) 
def load_industry_mapping():
    global pro
    if pro is None: return {}
    try:
        all_members = []
        load_bar = st.progress(0, text="åŠ è½½è¡Œä¸šæ•°æ®...")
        codes = list(SW_INDUSTRIES.keys())
        for i, idx_code in enumerate(codes):
            df = pro.index_member(index_code=idx_code, is_new='Y')
            if not df.empty:
                df['industry_name'] = SW_INDUSTRIES[idx_code]
                all_members.append(df[['con_code', 'industry_name']])
            time.sleep(0.02)
            load_bar.progress((i + 1) / len(codes))
        load_bar.empty()
        if not all_members: return {}
        full_df = pd.concat(all_members)
        full_df = full_df.drop_duplicates(subset=['con_code'])
        return dict(zip(full_df['con_code'], full_df['industry_name']))
    except: return {}

# ---------------------------
# 3. ç¾è‚¡æ•°æ®å¤„ç† (åŒæ¨¡ä¸‹è½½)
# ---------------------------
@st.cache_data(ttl=3600*12)
def get_us_history_data(start_date, end_date):
    global pro
    
    # æ”¾å®½æ—¥æœŸèŒƒå›´
    real_start = (datetime.strptime(start_date, "%Y%m%d") - timedelta(days=20)).strftime("%Y%m%d")
    real_end = (datetime.strptime(end_date, "%Y%m%d") + timedelta(days=5)).strftime("%Y%m%d")
    
    all_us_data = []
    failed_tickers = []
    
    # ---------------------------
    # é˜¶æ®µä¸€ï¼šä¼˜å…ˆå°è¯• Tushare
    # ---------------------------
    if pro is not None:
        bar = st.progress(0, text="å°è¯• Tushare ä¸‹è½½ç¾è‚¡æ•°æ®...")
        for i, ticker in enumerate(US_TICKERS):
            try:
                # å°è¯•çº¯ä»£ç  (e.g. XLK)
                df = safe_get('us_daily', ts_code=ticker, start_date=real_start, end_date=real_end)
                if df.empty:
                    # å¤±è´¥åˆ™å°è¯•åŠ  .US åç¼€ (e.g. XLK.US)
                    df = safe_get('us_daily', ts_code=ticker+'.US', start_date=real_start, end_date=real_end)
                
                if not df.empty:
                    df = df[['trade_date', 'ts_code', 'pct_change']]
                    # ç»Ÿä¸€ä»£ç æ ¼å¼ï¼Œå»æ‰å¯èƒ½å­˜åœ¨çš„åç¼€ï¼Œæ–¹ä¾¿åç»­å¤„ç†
                    df['ts_code'] = ticker 
                    all_us_data.append(df)
                else:
                    failed_tickers.append(ticker)
            except: 
                failed_tickers.append(ticker)
            bar.progress((i+1)/len(US_TICKERS))
        bar.empty()
    else:
        failed_tickers = US_TICKERS

    # ---------------------------
    # é˜¶æ®µäºŒï¼šè¡¥æ•‘æªæ–½ (YFinance)
    # ---------------------------
    if failed_tickers and yf:
        st.warning(f"Tushare éƒ¨åˆ†æ•°æ®è·å–å¤±è´¥ ({len(failed_tickers)}ä¸ª)ï¼Œæ­£åœ¨è°ƒç”¨ YFinance è¡¥å…¨...")
        try:
            # YFinance éœ€è¦ .US åç¼€å—ï¼Ÿä¸éœ€è¦ï¼Œä½†éœ€è¦ç¡®ä¿ç½‘ç»œ
            # æ‰¹é‡ä¸‹è½½
            yf_data = yf.download(failed_tickers, start=datetime.strptime(real_start, "%Y%m%d"), end=datetime.strptime(real_end, "%Y%m%d"), progress=False)['Close']
            
            if not yf_data.empty:
                # è®¡ç®—æ¶¨è·Œå¹…
                pct_df = yf_data.pct_change() * 100
                # é‡æ„æ ¼å¼
                for ticker in failed_tickers:
                    if ticker in pct_df.columns:
                        single_df = pct_df[[ticker]].dropna().reset_index()
                        single_df.columns = ['trade_date', 'pct_change']
                        single_df['ts_code'] = ticker
                        single_df['trade_date'] = single_df['trade_date'].dt.strftime('%Y%m%d')
                        all_us_data.append(single_df)
        except Exception as e:
            st.error(f"YFinance è¡¥å…¨ä¹Ÿå¤±è´¥äº†: {e}")

    if not all_us_data: return pd.DataFrame()
    
    full_df = pd.concat(all_us_data)
    # Pivot: Index=Date, Cols=Ticker
    pivot_df = full_df.pivot_table(index='trade_date', columns='ts_code', values='pct_change')
    return pivot_df.sort_index()

def get_hot_inds_for_date(trade_date_str, top_n=3):
    global GLOBAL_US_DATA
    if GLOBAL_US_DATA.empty: return []
    
    # æ‰¾å°äºå½“å‰æ—¥æœŸçš„æœ€è¿‘ä¸€ä¸ªç¾è‚¡äº¤æ˜“æ—¥
    valid_dates = GLOBAL_US_DATA.index[GLOBAL_US_DATA.index < trade_date_str]
    if len(valid_dates) == 0: return []
    
    target_us_date = valid_dates[-1]
    
    row = GLOBAL_US_DATA.loc[target_us_date].dropna()
    if row.empty: return []
    
    top_etfs = row.sort_values(ascending=False).head(top_n)
    
    target_inds = set()
    for etf_code, pct in top_etfs.items():
        # è¿™é‡Œä¸å¼ºåˆ¶è¦æ±‚ >0ï¼Œå› ä¸ºå¦‚æœæ˜¯æ™®è·Œï¼Œä¹Ÿåº”è¯¥é€‰è·Œå¾—æœ€å°‘çš„ï¼ˆç›¸å¯¹å¼ºåŠ¿ï¼‰
        # æˆ–è€…ä½ åªæƒ³åšä¸Šæ¶¨çš„ï¼Ÿä¸€èˆ¬ç›¸å¯¹å¼ºåº¦è¶³å¤Ÿ
        info = US_ETF_MAP.get(etf_code)
        if info:
            target_inds.update(info['cn_inds'])
                
    return list(target_inds)

# ---------------------------
# 4. æ•°æ®ä¸‹è½½ä¸ç¼“å­˜
# ---------------------------
CACHE_FILE_NAME = "market_data_cache_v15.pkl"

def get_all_historical_data(trade_days_list, use_cache=True):
    global GLOBAL_ADJ_FACTOR, GLOBAL_DAILY_RAW, GLOBAL_QFQ_BASE_FACTORS, GLOBAL_STOCK_INDUSTRY, GLOBAL_US_DATA
    
    GLOBAL_STOCK_INDUSTRY = load_industry_mapping()

    if use_cache and os.path.exists(CACHE_FILE_NAME):
        st.success("âš¡ æé€ŸåŠ è½½æœ¬åœ°ç¼“å­˜...")
        try:
            with open(CACHE_FILE_NAME, 'rb') as f:
                d = pickle.load(f)
                GLOBAL_ADJ_FACTOR = d['adj']
                GLOBAL_DAILY_RAW = d['daily']
                GLOBAL_US_DATA = d.get('us_data', pd.DataFrame())
            
            latest = GLOBAL_ADJ_FACTOR.index.get_level_values('trade_date').max()
            if latest:
                try: GLOBAL_QFQ_BASE_FACTORS = GLOBAL_ADJ_FACTOR.loc[(slice(None), latest), 'adj_factor'].droplevel(1).to_dict()
                except: pass
            
            if GLOBAL_US_DATA.empty:
                st.warning("ç¼“å­˜å®Œç¾è‚¡æ•°æ®ç¼ºå¤±ï¼Œæ­£åœ¨è¡¥å……...")
                start_d = min(trade_days_list)
                end_d = max(trade_days_list)
                GLOBAL_US_DATA = get_us_history_data(start_d, end_d)
                with open(CACHE_FILE_NAME, 'wb') as f: 
                    pickle.dump({'adj': GLOBAL_ADJ_FACTOR, 'daily': GLOBAL_DAILY_RAW, 'us_data': GLOBAL_US_DATA}, f)
            
            return True
        except: 
            os.remove(CACHE_FILE_NAME)

    latest = max(trade_days_list) 
    earliest = min(trade_days_list)
    s_dt = datetime.strptime(earliest, "%Y%m%d") - timedelta(days=200)
    e_dt = datetime.strptime(latest, "%Y%m%d") + timedelta(days=30)
    start_date = s_dt.strftime("%Y%m%d")
    end_date = e_dt.strftime("%Y%m%d")
    
    cal = safe_get('trade_cal', start_date=start_date, end_date=end_date, is_open='1')
    if cal.empty: return False
    all_dates = cal['cal_date'].tolist()
    
    st.info(f"ğŸ“¡ æ­£åœ¨ä¸‹è½½æ•°æ® (Aè‚¡ + ç¾è‚¡)...")
    
    adj_list, daily_list = [], []
    def fetch_worker(date): return fetch_and_cache_daily_data(date)
    
    bar = st.progress(0)
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        f2d = {executor.submit(fetch_worker, d): d for d in all_dates}
        for i, f in enumerate(concurrent.futures.as_completed(f2d)):
            try:
                d = f.result()
                if not d['adj'].empty: adj_list.append(d['adj'])
                if not d['daily'].empty: daily_list.append(d['daily'])
            except: pass
            if i%10==0: bar.progress((i+1)/len(all_dates))
    bar.empty()
    
    # ä¸‹è½½ç¾è‚¡
    GLOBAL_US_DATA = get_us_history_data(start_date, end_date)
    
    if not daily_list: return False
    with st.spinner("æ„å»ºç´¢å¼•..."):
        GLOBAL_ADJ_FACTOR = pd.concat(adj_list).drop_duplicates(subset=['ts_code', 'trade_date']).set_index(['ts_code', 'trade_date']).sort_index()
        GLOBAL_ADJ_FACTOR['adj_factor'] = pd.to_numeric(GLOBAL_ADJ_FACTOR['adj_factor'], errors='coerce').fillna(0)
        GLOBAL_DAILY_RAW = pd.concat(daily_list).drop_duplicates(subset=['ts_code', 'trade_date']).set_index(['ts_code', 'trade_date']).sort_index()
        
        latest_d = GLOBAL_ADJ_FACTOR.index.get_level_values('trade_date').max()
        if latest_d:
            try: GLOBAL_QFQ_BASE_FACTORS = GLOBAL_ADJ_FACTOR.loc[(slice(None), latest_d), 'adj_factor'].droplevel(1).to_dict()
            except: pass
            
        try:
            with open(CACHE_FILE_NAME, 'wb') as f: 
                pickle.dump({'adj': GLOBAL_ADJ_FACTOR, 'daily': GLOBAL_DAILY_RAW, 'us_data': GLOBAL_US_DATA}, f)
        except: pass
    return True

# ---------------------------
# 5. æ ¸å¿ƒè®¡ç®—
# ---------------------------
def get_qfq_data_v4_optimized_final(ts_code, start_date, end_date):
    global GLOBAL_DAILY_RAW, GLOBAL_ADJ_FACTOR, GLOBAL_QFQ_BASE_FACTORS
    if GLOBAL_DAILY_RAW.empty: return pd.DataFrame()
    latest_adj = GLOBAL_QFQ_BASE_FACTORS.get(ts_code, np.nan)
    if pd.isna(latest_adj): return pd.DataFrame()
    try:
        d = GLOBAL_DAILY_RAW.loc[ts_code]
        d = d[(d.index >= start_date) & (d.index <= end_date)]
        a = GLOBAL_ADJ_FACTOR.loc[ts_code]['adj_factor']
        a = a[(a.index >= start_date) & (a.index <= end_date)]
    except: return pd.DataFrame()
    if d.empty or a.empty: return pd.DataFrame()
    df = d.merge(a.rename('adj_factor'), left_index=True, right_index=True, how='left').dropna()
    for c in ['open','high','low','close']: df[c+'_qfq'] = df[c] * df['adj_factor'] / latest_adj
    df = df.reset_index().rename(columns={'trade_date':'trade_date_str'}).sort_values('trade_date_str').set_index('trade_date_str')
    for c in ['open','high','low','close']: df[c] = df[c+'_qfq']
    return df[['open','high','low','close','vol']]

def get_future_prices(ts_code, selection_date, d0_qfq_close, days_ahead=[1, 3, 5]):
    d0 = datetime.strptime(selection_date, "%Y%m%d")
    s = (d0 + timedelta(days=1)).strftime("%Y%m%d")
    e = (d0 + timedelta(days=15)).strftime("%Y%m%d")
    h = get_qfq_data_v4_optimized_final(ts_code, start_date=s, end_date=e)
    res = {}
    if h.empty: return res
    h['open'] = pd.to_numeric(h['open'], errors='coerce')
    h['high'] = pd.to_numeric(h['high'], errors='coerce')
    h['close'] = pd.to_numeric(h['close'], errors='coerce')
    
    d1 = h.iloc[0]
    next_open = d1['open']
    next_high = d1['high']
    
    if next_open <= d0_qfq_close: return res
    target_buy = next_open * 1.015
    if next_high < target_buy: return res
    
    for n in days_ahead:
        col = f'Return_D{n}'
        if len(h) >= n:
            res[col] = (h.iloc[n-1]['close'] - target_buy)/target_buy * 100
    return res

def compute_indicators(ts_code, end_date):
    s = (datetime.strptime(end_date, "%Y%m%d") - timedelta(days=150)).strftime("%Y%m%d")
    df = get_qfq_data_v4_optimized_final(ts_code, s, end_date)
    if df.empty or len(df)<26: return {}
    df['pct'] = df['close'].pct_change().fillna(0)*100
    res = {'last_close': df['close'].iloc[-1], 'last_high': df['high'].iloc[-1]}
    
    ema12 = df['close'].ewm(span=12).mean()
    ema26 = df['close'].ewm(span=26).mean()
    diff = ema12 - ema26
    dea = diff.ewm(span=9).mean()
    res['macd_val'] = ((diff-dea)*2).iloc[-1]
    
    ma20 = df['close'].tail(20).mean()
    res['ma20'] = ma20
    if ma20 > 0: res['bias_20'] = (res['last_close']-ma20)/ma20*100
    else: res['bias_20'] = 0
    
    delta = df['close'].diff()
    gain = (delta.where(delta>0, 0)).ewm(alpha=1/12).mean()
    loss = (-delta.where(delta<0, 0)).ewm(alpha=1/12).mean()
    rs = gain/(loss+1e-9)
    res['rsi_12'] = 100 - (100/(1+rs)).iloc[-1]
    return res

def get_market_state(trade_date):
    s = (datetime.strptime(trade_date, "%Y%m%d") - timedelta(days=40)).strftime("%Y%m%d")
    i = safe_get('daily', ts_code='000300.SH', start_date=s, end_date=trade_date, is_index=True)
    if i.empty or len(i)<20: return 'Weak'
    return 'Strong' if i.iloc[-1]['close'] > i['close'].tail(20).mean() else 'Weak'

# ---------------------------
# 6. å›æµ‹é€»è¾‘
# ---------------------------
def run_backtest_for_a_day(last_trade, TOP_BACKTEST, FINAL_POOL, MAX_UPPER_SHADOW, MAX_TURNOVER_RATE, RSI_LIMIT, CHIP_MIN_WIN_RATE, SECTOR_THRESHOLD, MIN_MV, MAX_MV, MAX_PREV_PCT, MIN_PRICE, USE_US_MAP, TOP_N_US):
    global GLOBAL_STOCK_INDUSTRY
    
    target_inds = []
    if USE_US_MAP:
        target_inds = get_hot_inds_for_date(last_trade, top_n=TOP_N_US)
    
    market_state = get_market_state(last_trade)
    daily = safe_get('daily', trade_date=last_trade)
    if daily.empty: return pd.DataFrame(), "æ— æ•°æ®"
    
    base = safe_get('stock_basic', list_status='L')
    df = daily.merge(base[['ts_code','name']], on='ts_code')
    
    daily_basic = safe_get('daily_basic', trade_date=last_trade)
    if not daily_basic.empty:
        df = df.merge(daily_basic[['ts_code','turnover_rate','circ_mv']], on='ts_code', how='left')
    else: df['turnover_rate'] = 0; df['circ_mv'] = 0
        
    mf = safe_get('moneyflow', trade_date=last_trade)
    if not mf.empty: df = df.merge(mf[['ts_code','net_mf_amount']], on='ts_code', how='left').rename(columns={'net_mf_amount':'net_mf'})
    else: df['net_mf'] = 0
    df['net_mf'] = df['net_mf'].fillna(0)
    
    df = df[~df['name'].str.contains('ST|é€€')]
    df = df[(df['close']>=MIN_PRICE) & (df['close']<=2000) & (df['turnover_rate']<=MAX_TURNOVER_RATE)]
    df = df[(df['circ_mv']/10000 >= MIN_MV) & (df['circ_mv']/10000 <= MAX_MV)]
    
    if df.empty: return pd.DataFrame(), "è¿‡æ»¤ç©º"
    cands = df.sort_values('pct_chg', ascending=False).head(FINAL_POOL)
    recs = []
    
    for row in cands.itertuples():
        ind_name = GLOBAL_STOCK_INDUSTRY.get(row.ts_code)
        if USE_US_MAP:
            # å¦‚æœæœ‰ç¾è‚¡æ•°æ®ï¼Œä¸¥æ ¼æ‰§è¡Œæ˜ å°„
            if target_inds and (ind_name not in target_inds): continue
        
        if row.pct_chg > MAX_PREV_PCT: continue
        
        ind = compute_indicators(row.ts_code, last_trade)
        if not ind: continue
        
        if ind.get('bias_20', 0) > 20: continue
        if ind['last_close'] < ind['ma20']: continue
        if market_state == 'Weak' and ind['rsi_12'] > RSI_LIMIT: continue
        
        upper = (ind['last_high'] - ind['last_close'])/ind['last_close']*100
        if upper > MAX_UPPER_SHADOW: continue
        
        fut = get_future_prices(row.ts_code, last_trade, ind['last_close'])
        
        recs.append({
            'ts_code': row.ts_code, 'name': row.name, 'rsi': ind['rsi_12'],
            'macd': ind['macd_val'], 'net_mf': row.net_mf, 'bias_20': ind.get('bias_20',0),
            'Return_D1 (%)': fut.get('Return_D1'), 'Return_D3 (%)': fut.get('Return_D3'),
            'Return_D5 (%)': fut.get('Return_D5'), 'market_state': market_state,
            'winner_rate': 80,
            'us_link': 'Yes' if (USE_US_MAP and target_inds) else 'No'
        })

    if not recs: return pd.DataFrame(), "æ— æ ‡çš„"
    fdf = pd.DataFrame(recs)
    
    def score(r):
        s = r['macd']*1000 + r['net_mf']/10000
        if 60<=r['rsi']<=85: s+=2000
        elif r['rsi']>90: s-=1000
        if r['bias_20']<10: s+=1000
        if r['market_state']=='Strong' and r['rsi']>RSI_LIMIT: s-=500
        return s
    
    fdf['Score'] = fdf.apply(score, axis=1)
    final = fdf.sort_values('Score', ascending=False).head(TOP_BACKTEST)
    final.insert(0, 'Rank', range(1, len(final)+1))
    return final, None

# ---------------------------
# 7. UI
# ---------------------------
with st.sidebar:
    st.header("V30.12.15 çœŸå®å›æµ‹ä¿®å¤ç‰ˆ")
    backtest_date_end = st.date_input("åˆ†ææˆªæ­¢æ—¥æœŸ", value=datetime.now().date())
    BACKTEST_DAYS = st.number_input("åˆ†æå¤©æ•°", value=30, step=1)
    TOP_BACKTEST = st.number_input("æ¯æ—¥ä¼˜é€‰ TopK", value=5)
    
    st.markdown("---")
    st.subheader("ğŸ‡ºğŸ‡¸ ç¾è‚¡è”åŠ¨ç›‘æ§")
    USE_US_MAP = st.checkbox("å¼€å¯ç¾è‚¡å†å²å›æº¯", value=True)
    TOP_N_US = st.slider("é€‰å–ç¾è‚¡ Top N", 1, 5, 3) 
    
    # æ•°æ®çŠ¶æ€ç›‘æ§
    if not GLOBAL_US_DATA.empty:
        st.success(f"âœ… ç¾è‚¡å†å²æ•°æ®å·²å°±ç»ª ({len(GLOBAL_US_DATA)} å¤©)")
    else:
        st.info("âŒ› å¾…ä¸‹è½½ç¾è‚¡æ•°æ®...")

    st.markdown("---")
    RESUME_CHECKPOINT = st.checkbox("ğŸ”¥ å¼€å¯æ–­ç‚¹ç»­ä¼ ", value=True)
    if st.button("ğŸ—‘ï¸ æ¸…é™¤è¡Œæƒ…ç¼“å­˜"):
        if os.path.exists(CACHE_FILE_NAME): os.remove(CACHE_FILE_NAME)
    CHECKPOINT_FILE = "backtest_checkpoint_v15.csv"
    
    # åŸºç¡€å‚æ•°
    MIN_PRICE = st.number_input("æœ€ä½è‚¡ä»·", value=10.0) 
    MIN_MV = st.number_input("æœ€å°å¸‚å€¼(äº¿)", value=30.0) 
    MAX_MV = st.number_input("æœ€å¤§å¸‚å€¼(äº¿)", value=1000.0)
    CHIP_MIN_WIN_RATE = st.number_input("æœ€ä½è·åˆ©ç›˜ (%)", value=70.0)
    MAX_PREV_PCT = st.number_input("æ˜¨æ—¥æœ€å¤§æ¶¨å¹…é™åˆ¶ (%)", value=19.0)
    RSI_LIMIT = st.number_input("RSI æ‹¦æˆªçº¿", value=100.0)
    SECTOR_THRESHOLD = st.number_input("æ¿å—æ¶¨å¹… (%)", value=1.5)
    MAX_UPPER_SHADOW = st.number_input("ä¸Šå½±çº¿ (%)", value=5.0)
    MAX_TURNOVER_RATE = st.number_input("æ¢æ‰‹ç‡ (%)", value=20.0)

TS_TOKEN = st.text_input("Tushare Token", type="password")
if not TS_TOKEN: st.stop()
ts.set_token(TS_TOKEN)
pro = ts.pro_api()

if st.button(f"ğŸš€ å¯åŠ¨ä¿®å¤ç‰ˆå›æµ‹"):
    processed_dates = set()
    results = []
    
    if RESUME_CHECKPOINT and os.path.exists(CHECKPOINT_FILE):
        try:
            existing_df = pd.read_csv(CHECKPOINT_FILE)
            existing_df['Trade_Date'] = existing_df['Trade_Date'].astype(str)
            processed_dates = set(existing_df['Trade_Date'].unique())
            results.append(existing_df)
            st.success(f"âœ… æ–­ç‚¹ç»­ä¼ ï¼šè·³è¿‡ {len(processed_dates)} å¤©")
        except: pass
    else:
        if os.path.exists(CHECKPOINT_FILE): os.remove(CHECKPOINT_FILE)
    
    trade_days_list = get_trade_days(backtest_date_end.strftime("%Y%m%d"), int(BACKTEST_DAYS))
    if not trade_days_list: st.stop()
        
    dates_to_run = [d for d in trade_days_list if d not in processed_dates]
    
    if not dates_to_run:
        st.success("ğŸ‰ åˆ†æå®Œæ¯•")
    else:
        # ä¸‹è½½æ•°æ® (å«ç¾è‚¡åŒæ¨¡ä¸‹è½½)
        if not get_all_historical_data(trade_days_list, use_cache=True): st.stop()
            
        bar = st.progress(0, text="å¯åŠ¨å¼•æ“...")
        for i, date in enumerate(dates_to_run):
            res, err = run_backtest_for_a_day(
                date, int(TOP_BACKTEST), 100, MAX_UPPER_SHADOW, MAX_TURNOVER_RATE, 
                RSI_LIMIT, CHIP_MIN_WIN_RATE, SECTOR_THRESHOLD, MIN_MV, MAX_MV, 
                MAX_PREV_PCT, MIN_PRICE, 
                USE_US_MAP, TOP_N_US
            )
            if not res.empty:
                res['Trade_Date'] = date
                is_first = not os.path.exists(CHECKPOINT_FILE)
                res.to_csv(CHECKPOINT_FILE, mode='a', index=False, header=is_first, encoding='utf-8-sig')
                results.append(res)
            bar.progress((i+1)/len(dates_to_run), text=f"åˆ†æä¸­: {date}")
        bar.empty()
    
    if results:
        all_res = pd.concat(results)
        all_res = all_res[all_res['Rank'] <= int(TOP_BACKTEST)]
        all_res['Trade_Date'] = all_res['Trade_Date'].astype(str)
        all_res = all_res.sort_values(['Trade_Date', 'Rank'], ascending=[False, True])
        
        st.header(f"ğŸ“Š ç»Ÿè®¡ä»ªè¡¨ç›˜ (ä¿®å¤ç‰ˆ - Top{TOP_N_US})")
        cols = st.columns(3)
        for idx, n in enumerate([1, 3, 5]):
            col_name = f'Return_D{n} (%)'
            valid = all_res.dropna(subset=[col_name]) 
            if not valid.empty:
                avg = valid[col_name].mean()
                win = (valid[col_name] > 0).mean() * 100
                cols[idx].metric(f"D+{n} å‡ç›Š / èƒœç‡", f"{avg:.2f}% / {win:.1f}%")
        
        st.dataframe(all_res, use_container_width=True)
        csv = all_res.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ", csv, f"export_v15_fixed.csv", "text/csv")
    else:
        st.warning("âš ï¸ æ²¡æœ‰ç»“æœ")
