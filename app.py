# -*- coding: utf-8 -*-
"""
é€‰è‚¡ç‹ Â· V30.12.18 æ¶¨å¹…ç†”æ–­ç‰ˆ
------------------------------------------------
åŸºäº tb1.txt ä¿®æ”¹
æ ¸å¿ƒæ”¹åŠ¨ï¼š
1. ã€ç´¯è®¡æ¶¨å¹…ç†”æ–­ã€‘æ–°å¢ æ–¹æ¡ˆA é£æ§ï¼š
   - 5æ—¥ç´¯è®¡æ¶¨å¹… > 40% -> ç¦ä¹°
   - 10æ—¥ç´¯è®¡æ¶¨å¹… > 70% -> ç¦ä¹°
2. ã€ä¿ç•™åŸå‘³ã€‘å…¶ä»–é€»è¾‘ï¼ˆRankç®—æ³•ã€RSIã€MACDï¼‰ä¸ tb1 å®Œå…¨ä¸€è‡´ã€‚
------------------------------------------------
"""

import streamlit as st
import pandas as pd
import numpy as np
import tushare as ts
from datetime import datetime, timedelta
import warnings
import time
import concurrent.futures 
import os
import pickle

warnings.filterwarnings("ignore")

# ---------------------------
# å…¨å±€å˜é‡åˆå§‹åŒ–
# ---------------------------
pro = None 
GLOBAL_ADJ_FACTOR = pd.DataFrame() 
GLOBAL_DAILY_RAW = pd.DataFrame() 
GLOBAL_QFQ_BASE_FACTORS = {} 
GLOBAL_STOCK_INDUSTRY = {} 

# ---------------------------
# é¡µé¢è®¾ç½®
# ---------------------------
st.set_page_config(page_title="é€‰è‚¡ç‹ V30.12.18 æ¶¨å¹…ç†”æ–­ç‰ˆ", layout="wide")
st.title("é€‰è‚¡ç‹ V30.12.18ï¼šæ¶¨å¹…ç†”æ–­ç‰ˆ (æ–¹æ¡ˆA)")

# ---------------------------
# åŸºç¡€ API å‡½æ•°
# ---------------------------
@st.cache_data(ttl=3600*12) 
def safe_get(func_name, **kwargs):
    global pro
    if pro is None: 
        return pd.DataFrame(columns=['ts_code']) 
   
    func = getattr(pro, func_name) 
    try:
        for _ in range(3):
            try:
                if kwargs.get('is_index'):
                    df = pro.index_daily(**kwargs)
                else:
                    df = func(**kwargs)
                
                if df is not None and not df.empty:
                    return df
                time.sleep(0.5)
            except:
                time.sleep(1)
                continue
        return pd.DataFrame(columns=['ts_code']) 
    except Exception as e:
        return pd.DataFrame(columns=['ts_code'])

def get_trade_days(end_date_str, num_days):
    lookback_days = max(num_days * 3, 365) 
    start_date = (datetime.strptime(end_date_str, "%Y%m%d") - timedelta(days=lookback_days)).strftime("%Y%m%d")
    cal = safe_get('trade_cal', start_date=start_date, end_date=end_date_str)
    if cal.empty: return []
    trade_days_df = cal[cal['is_open'] == 1].sort_values('cal_date', ascending=False)
    return trade_days_df[trade_days_df['cal_date'] <= end_date_str]['cal_date'].head(num_days).tolist()

@st.cache_data(ttl=3600*24)
def fetch_and_cache_daily_data(date):
    adj = safe_get('adj_factor', trade_date=date)
    daily = safe_get('daily', trade_date=date)
    return {'adj': adj, 'daily': daily}

# ---------------------------
# æ•°æ®ç¼“å­˜é€»è¾‘ (æœ¬åœ°åŒ–)
# ---------------------------
CACHE_FILE_NAME = "market_data_cache.pkl"

def get_all_historical_data(trade_days_list, use_cache=True):
    global GLOBAL_ADJ_FACTOR, GLOBAL_DAILY_RAW, GLOBAL_QFQ_BASE_FACTORS
    
    # 1. å°è¯•è¯»å–æœ¬åœ°ç¼“å­˜
    if use_cache and os.path.exists(CACHE_FILE_NAME):
        st.success("âš¡ å‘ç°æœ¬åœ°ç¼“å­˜ï¼Œæ­£åœ¨æé€ŸåŠ è½½...")
        try:
            with open(CACHE_FILE_NAME, 'rb') as f:
                d = pickle.load(f)
                GLOBAL_ADJ_FACTOR = d['adj']
                GLOBAL_DAILY_RAW = d['daily']
            
            # æ¢å¤åŸºå‡†å¤æƒå› å­
            latest = GLOBAL_ADJ_FACTOR.index.get_level_values('trade_date').max()
            if latest:
                try:
                    GLOBAL_QFQ_BASE_FACTORS = GLOBAL_ADJ_FACTOR.loc[(slice(None), latest), 'adj_factor'].droplevel(1).to_dict()
                except: pass
            return True
        except Exception as e:
            st.warning(f"ç¼“å­˜æ–‡ä»¶æŸåï¼Œå°†é‡æ–°ä¸‹è½½: {e}")
            os.remove(CACHE_FILE_NAME)

    # 2. å¦‚æœæ²¡æœ‰ç¼“å­˜ï¼Œåˆ™ä¸‹è½½
    latest_trade_date = max(trade_days_list) 
    earliest_trade_date = min(trade_days_list)
    
    # å‘å‰å¤šå–æ•°æ®ä»¥ç¡®ä¿è®¡ç®—æŒ‡æ ‡
    start_date_dt = datetime.strptime(earliest_trade_date, "%Y%m%d") - timedelta(days=200)
    end_date_dt = datetime.strptime(latest_trade_date, "%Y%m%d") + timedelta(days=30)
    
    start_date = start_date_dt.strftime("%Y%m%d")
    end_date = end_date_dt.strftime("%Y%m%d")
    
    cal = safe_get('trade_cal', start_date=start_date, end_date=end_date, is_open='1')
    if cal.empty: return False
    all_dates = cal['cal_date'].tolist()
    
    st.info(f"ğŸ“¡ [é¦–æ¬¡è¿è¡Œ] æ­£åœ¨ä¸‹è½½å…¨å¸‚åœºæ•°æ®: {start_date} è‡³ {end_date}...")
    
    adj_list = []
    daily_list = []
    
    def fetch_worker(date):
        return fetch_and_cache_daily_data(date)
    
    bar = st.progress(0)
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        f2d = {executor.submit(fetch_worker, d): d for d in all_dates}
        for i, f in enumerate(concurrent.futures.as_completed(f2d)):
            try:
                d = f.result()
                if not d['adj'].empty: adj_list.append(d['adj'])
                if not d['daily'].empty: daily_list.append(d['daily'])
            except: pass
            if i % 10 == 0:
                bar.progress((i + 1) / len(all_dates))
    bar.empty()
    
    if not daily_list: return False
    
    with st.spinner("æ­£åœ¨æ„å»ºæ•°æ®ç´¢å¼•å¹¶å†™å…¥ç¡¬ç›˜..."):
        # åˆå¹¶
        GLOBAL_ADJ_FACTOR = pd.concat(adj_list).drop_duplicates(subset=['ts_code', 'trade_date']).set_index(['ts_code', 'trade_date']).sort_index()
        GLOBAL_ADJ_FACTOR['adj_factor'] = pd.to_numeric(GLOBAL_ADJ_FACTOR['adj_factor'], errors='coerce').fillna(0)
        
        GLOBAL_DAILY_RAW = pd.concat(daily_list).drop_duplicates(subset=['ts_code', 'trade_date']).set_index(['ts_code', 'trade_date']).sort_index()
        
        # é¢„è®¡ç®—æœ€æ–°å¤æƒå› å­
        latest = GLOBAL_ADJ_FACTOR.index.get_level_values('trade_date').max()
        if latest:
            try:
                GLOBAL_QFQ_BASE_FACTORS = GLOBAL_ADJ_FACTOR.loc[(slice(None), latest), 'adj_factor'].droplevel(1).to_dict()
            except: pass
            
        # å†™å…¥ç¼“å­˜
        try:
            with open(CACHE_FILE_NAME, 'wb') as f:
                pickle.dump({'adj': GLOBAL_ADJ_FACTOR, 'daily': GLOBAL_DAILY_RAW}, f)
        except Exception as e:
            st.error(f"å†™å…¥ç¼“å­˜å¤±è´¥: {e}")
            
    return True

# ---------------------------
# å¤æƒæ•°æ®è®¡ç®— (æé€Ÿç‰ˆ)
# ---------------------------
def get_qfq_data_v4_optimized_final(ts_code, start_date, end_date):
    global GLOBAL_DAILY_RAW, GLOBAL_ADJ_FACTOR, GLOBAL_QFQ_BASE_FACTORS
    
    if GLOBAL_DAILY_RAW.empty: return pd.DataFrame()
    
    # å¿«é€Ÿè·å–æœ€æ–°å¤æƒå› å­
    latest_adj_factor = GLOBAL_QFQ_BASE_FACTORS.get(ts_code, np.nan)
    if pd.isna(latest_adj_factor): return pd.DataFrame() 

    try:
        # åˆ‡ç‰‡è·å–ä¸ªè‚¡æ•°æ®
        daily_df = GLOBAL_DAILY_RAW.loc[ts_code]
        daily_df = daily_df.loc[(daily_df.index >= start_date) & (daily_df.index <= end_date)]
        
        adj_series = GLOBAL_ADJ_FACTOR.loc[ts_code]['adj_factor']
        adj_series = adj_series.loc[(adj_series.index >= start_date) & (adj_series.index <= end_date)]
    except KeyError:
        return pd.DataFrame()
    
    if daily_df.empty or adj_series.empty: return pd.DataFrame()
    
    # åˆå¹¶
    df = daily_df.merge(adj_series.rename('adj_factor'), left_index=True, right_index=True, how='left')
    df = df.dropna(subset=['adj_factor'])
    
    # å‰å¤æƒè®¡ç®—: Price_QFQ = Price * (Adj / Latest_Adj)
    # å‘é‡åŒ–è®¡ç®—æ¯”å¾ªç¯å¿«
    for col in ['open', 'high', 'low', 'close', 'pre_close']:
        if col in df.columns:
            df[col + '_qfq'] = df[col] * df['adj_factor'] / latest_adj_factor
    
    df = df.reset_index().rename(columns={'trade_date': 'trade_date_str'})
    df = df.sort_values('trade_date_str').set_index('trade_date_str')
    
    # æ›¿æ¢åŸåˆ—
    for col in ['open', 'high', 'low', 'close']:
        df[col] = df[col + '_qfq']
        
    return df[['open', 'high', 'low', 'close', 'vol']].copy() 

# ---------------------------
# æŒ‡æ ‡è®¡ç®— (å«ç´¯è®¡æ¶¨å¹…)
# ---------------------------
def compute_indicators(ts_code, end_date):
    # å¤šå–ä¸€äº›æ•°æ®ç”¨äºè®¡ç®—ç´¯è®¡æ¶¨å¹…
    start_date = (datetime.strptime(end_date, "%Y%m%d") - timedelta(days=150)).strftime("%Y%m%d")
    df = get_qfq_data_v4_optimized_final(ts_code, start_date=start_date, end_date=end_date)
    
    res = {}
    if df.empty or len(df) < 26: return res 
    
    # åŸºç¡€æŒ‡æ ‡
    df['pct_chg'] = df['close'].pct_change().fillna(0) * 100 
    close = df['close']
    
    res['last_close'] = close.iloc[-1]
    res['last_open'] = df['open'].iloc[-1]
    res['last_high'] = df['high'].iloc[-1]
    res['last_low'] = df['low'].iloc[-1]
    
    # MACD
    ema12 = close.ewm(span=12, adjust=False).mean()
    ema26 = close.ewm(span=26, adjust=False).mean()
    diff = ema12 - ema26
    dea = diff.ewm(span=9, adjust=False).mean()
    res['macd_val'] = ((diff - dea) * 2).iloc[-1]
    
    # å‡çº¿
    res['ma20'] = close.tail(20).mean()
    res['ma60'] = close.tail(60).mean()
    
    # RSI
    delta = close.diff()
    gain = (delta.where(delta > 0, 0)).ewm(alpha=1/12, adjust=False).mean()
    loss = (-delta.where(delta < 0, 0)).ewm(alpha=1/12, adjust=False).mean()
    rs = gain / (loss + 1e-9)
    res['rsi_12'] = 100 - (100 / (1 + rs)).iloc[-1]
    
    # ã€æ–°å¢ã€‘ç´¯è®¡æ¶¨å¹…è®¡ç®— (ç”¨äºç†”æ–­)
    # 5æ—¥ç´¯è®¡æ¶¨å¹… = (Today_Close - Close_5_days_ago) / Close_5_days_ago
    if len(close) >= 6:
        res['pct_chg_5d'] = (close.iloc[-1] / close.iloc[-6] - 1) * 100
    else:
        res['pct_chg_5d'] = 0
        
    # 10æ—¥ç´¯è®¡æ¶¨å¹…
    if len(close) >= 11:
        res['pct_chg_10d'] = (close.iloc[-1] / close.iloc[-11] - 1) * 100
    else:
        res['pct_chg_10d'] = 0

    return res

def get_future_prices(ts_code, selection_date, d0_close, days_ahead=[1, 3, 5]):
    d0 = datetime.strptime(selection_date, "%Y%m%d")
    s = (d0 + timedelta(days=1)).strftime("%Y%m%d")
    e = (d0 + timedelta(days=20)).strftime("%Y%m%d")
    
    h = get_qfq_data_v4_optimized_final(ts_code, start_date=s, end_date=e)
    res = {}
    if h.empty: return res
    
    for n in days_ahead:
        col = f'Return_D{n}'
        if len(h) >= n:
            # æ”¶ç›Šç‡ = (Dn_Close - D0_Close) / D0_Close
            # æ³¨æ„ï¼šè¿™é‡Œè®¡ç®—çš„æ˜¯ä¹°å…¥åæŒæœ‰ N å¤©çš„æ”¶ç›Šï¼ŒåŸºå‡†æ˜¯é€‰è‚¡æ—¥çš„æ”¶ç›˜ä»·
            res[col] = (h.iloc[n-1]['close'] - d0_close) / d0_close * 100
    return res

def get_market_state(trade_date):
    # ç®€å•åˆ¤å®šï¼šå¤§ç›˜(æ²ªæ·±300) 20æ—¥å‡çº¿ä¹‹ä¸Šä¸ºå¼ºï¼Œä¹‹ä¸‹ä¸ºå¼±
    s = (datetime.strptime(trade_date, "%Y%m%d") - timedelta(days=60)).strftime("%Y%m%d")
    idx_df = safe_get('daily', ts_code='000300.SH', start_date=s, end_date=trade_date, is_index=True)
    if idx_df.empty or len(idx_df) < 20: return 'Weak'
    
    current_close = idx_df['close'].iloc[-1]
    ma20 = idx_df['close'].tail(20).mean()
    return 'Strong' if current_close > ma20 else 'Weak'

# ---------------------------
# å›æµ‹ä¸»é€»è¾‘
# ---------------------------
def run_backtest_for_a_day(last_trade, TOP_BACKTEST, FINAL_POOL, MAX_PREV_PCT, RSI_LIMIT, MAX_RET_5D, MAX_RET_10D):
    # 1. è·å–å¸‚åœºçŠ¶æ€
    market_state = get_market_state(last_trade)
    
    # 2. è·å–å½“æ—¥å…¨å¸‚åœºè¡Œæƒ…
    daily = safe_get('daily', trade_date=last_trade)
    if daily.empty: return pd.DataFrame(), "ä»Šæ—¥æ— æ•°æ®"
    
    # 3. åŸºç¡€è¿‡æ»¤
    base = safe_get('stock_basic', list_status='L')
    df = daily.merge(base[['ts_code','name','industry']], on='ts_code')
    
    # å»é™¤ ST å’Œ é€€å¸‚
    df = df[~df['name'].str.contains('ST|é€€')]
    
    # 4. åˆç­›: æ¶¨å¹…é™åºï¼Œå–å‰ FINAL_POOL å
    cands = df.sort_values('pct_chg', ascending=False).head(FINAL_POOL)
    
    recs = []
    for row in cands.itertuples():
        # [è¿‡æ»¤1] æ˜¨æ—¥/ä»Šæ—¥å•æ—¥æ¶¨å¹…è¿‡å¤§ (é˜²ä¸€å­—æ¿)
        if row.pct_chg > MAX_PREV_PCT: continue
        
        # è®¡ç®—æŒ‡æ ‡
        ind = compute_indicators(row.ts_code, last_trade)
        if not ind: continue
        
        # [è¿‡æ»¤2] RSI æ‹¦æˆª (é»˜è®¤100ï¼Œç›¸å½“äºä¸æ‹¦æˆª)
        d0_rsi = ind['rsi_12']
        if d0_rsi > RSI_LIMIT: continue

        # [è¿‡æ»¤3] ç´¯è®¡æ¶¨å¹…ç†”æ–­ (æ–¹æ¡ˆA)
        # 5æ—¥æ¶¨å¹…è¿‡å¤§ -> ç†”æ–­
        if ind.get('pct_chg_5d', 0) > MAX_RET_5D: continue
        # 10æ—¥æ¶¨å¹…è¿‡å¤§ -> ç†”æ–­
        if ind.get('pct_chg_10d', 0) > MAX_RET_10D: continue

        # è·å–æœªæ¥æ”¶ç›Š (ç”¨äºå›æµ‹éªŒè¯)
        fut = get_future_prices(row.ts_code, last_trade, ind['last_close'])
        
        # èµ„é‡‘æµ (éœ€è¦é¢å¤–è·å–ï¼Œè¿™é‡Œç®€åŒ–ä¸º 0 æˆ–éœ€è°ƒç”¨ moneyflow)
        # ä¸ºä¿æŒæé€Ÿç‰ˆé€Ÿåº¦ï¼Œæš‚ç”¨æˆäº¤é‡ä»£æ›¿èµ„é‡‘çƒ­åº¦ï¼Œæˆ–å¦‚æœå·²æœ‰ç¼“å­˜å¯ç”¨
        net_mf = 0 # ç®€åŒ–
        
        recs.append({
            'ts_code': row.ts_code, 
            'name': row.name, 
            'Close': ind['last_close'],
            'Pct_Chg': row.pct_chg,
            'rsi': d0_rsi,
            'macd': ind['macd_val'],
            'pct_chg_5d': ind.get('pct_chg_5d', 0),   # è®°å½•ä¸‹æ¥ä»¥ä¾¿æŸ¥çœ‹
            'pct_chg_10d': ind.get('pct_chg_10d', 0), # è®°å½•ä¸‹æ¥ä»¥ä¾¿æŸ¥çœ‹
            'Return_D1 (%)': fut.get('Return_D1'), 
            'Return_D3 (%)': fut.get('Return_D3'),
            'Return_D5 (%)': fut.get('Return_D5'),
            'market_state': market_state,
        })

    if not recs: return pd.DataFrame(), "æ— ç¬¦åˆæ ‡çš„"
    
    fdf = pd.DataFrame(recs)
    
    # ---------------------------
    # æ‰“åˆ†æ’åº (Rank æ ¸å¿ƒ)
    # ---------------------------
    def score(r):
        # åŸºç¡€åˆ†ï¼šMACD è¶Šå¼ºè¶Šå¥½
        s = r['macd'] * 100 
        
        # RSI åŠ åˆ†é¡¹ (å¼ºè€…æ’å¼º)
        if r['rsi'] > 70: s += 50
        
        # å¸‚åœºçŠ¶æ€ä¿®æ­£
        if r['market_state'] == 'Strong':
            if r['rsi'] > 80: s += 20
        else:
            if r['rsi'] > 85: s -= 50 # å¼±å¸‚ä¸åšè¶…ä¹°
            
        return s
    
    fdf['Score'] = fdf.apply(score, axis=1)
    
    # æ’åºå– Top K
    final = fdf.sort_values('Score', ascending=False).head(TOP_BACKTEST)
    final.insert(0, 'Rank', range(1, len(final)+1))
    
    return final, None

# ---------------------------
# UI ä¾§è¾¹æ 
# ---------------------------
with st.sidebar:
    st.header("V30.12.18 æ¶¨å¹…ç†”æ–­é…ç½®")
    
    # ç†”æ–­å‚æ•°é…ç½® (æ–¹æ¡ˆAé»˜è®¤å€¼)
    st.subheader("ğŸ›¡ï¸ ç´¯è®¡æ¶¨å¹…ç†”æ–­ (æ–¹æ¡ˆA)")
    MAX_RET_5D = st.number_input("5æ—¥ç´¯è®¡æ¶¨å¹…ä¸Šé™ (%)", value=40.0, help="è¶…è¿‡æ­¤å€¼åšå†³ä¸ä¹°")
    MAX_RET_10D = st.number_input("10æ—¥ç´¯è®¡æ¶¨å¹…ä¸Šé™ (%)", value=70.0, help="è¶…è¿‡æ­¤å€¼åšå†³ä¸ä¹°")
    
    st.markdown("---")
    
    backtest_date_end = st.date_input("åˆ†ææˆªæ­¢æ—¥æœŸ", value=datetime.now().date())
    BACKTEST_DAYS = st.number_input("å›æµ‹å¤©æ•°", value=10, step=1)
    TOP_BACKTEST = st.number_input("æ¯æ—¥ä¼˜é€‰ TopK", value=5)
    
    # ç¼“å­˜ç®¡ç†
    if st.button("ğŸ—‘ï¸ æ¸…é™¤è¡Œæƒ…ç¼“å­˜"):
        if os.path.exists(CACHE_FILE_NAME): os.remove(CACHE_FILE_NAME)
        st.success("ç¼“å­˜å·²æ¸…é™¤ï¼Œä¸‹æ¬¡è¿è¡Œå°†é‡æ–°ä¸‹è½½")
    
    RESUME_CHECKPOINT = st.checkbox("ğŸ”¥ å¼€å¯æ–­ç‚¹ç»­ä¼ ", value=True)
    CHECKPOINT_FILE = "backtest_checkpoint_v18.csv"

    # å…¶ä»–å‚æ•°
    MAX_PREV_PCT = st.number_input("æ˜¨æ—¥æœ€å¤§æ¶¨å¹…é™åˆ¶ (%)", value=19.0)
    RSI_LIMIT = st.number_input("RSI æ‹¦æˆªçº¿ (å»ºè®®100)", value=100.0)

# ---------------------------
# Tushare Token
# ---------------------------
TS_TOKEN = st.text_input("Tushare Token", type="password")
if not TS_TOKEN: st.stop()
ts.set_token(TS_TOKEN)
pro = ts.pro_api()

# ---------------------------
# ä¸»ç¨‹åº
# ---------------------------
if st.button(f"ğŸš€ å¯åŠ¨ V30.12.18 å›æµ‹"):
    
    # å¤„ç†æ–­ç‚¹ç»­ä¼ 
    processed_dates = set()
    results = []
    if RESUME_CHECKPOINT and os.path.exists(CHECKPOINT_FILE):
        try:
            existing_df = pd.read_csv(CHECKPOINT_FILE)
            existing_df['Trade_Date'] = existing_df['Trade_Date'].astype(str)
            processed_dates = set(existing_df['Trade_Date'].unique())
            results.append(existing_df)
            st.success(f"âœ… æ–­ç‚¹ç»­ä¼ ï¼šå·²è·³è¿‡ {len(processed_dates)} å¤©")
        except: pass
    else:
        if os.path.exists(CHECKPOINT_FILE): os.remove(CHECKPOINT_FILE)
    
    # è·å–äº¤æ˜“æ—¥å†
    trade_days_list = get_trade_days(backtest_date_end.strftime("%Y%m%d"), int(BACKTEST_DAYS))
    if not trade_days_list: st.stop()
        
    dates_to_run = [d for d in trade_days_list if d not in processed_dates]
    
    if not dates_to_run:
        st.success("ğŸ‰ åˆ†æå®Œæ¯• (æ‰€æœ‰æ—¥æœŸå·²åœ¨ç¼“å­˜ä¸­)")
    else:
        # 1. å‡†å¤‡æ•°æ®
        if not get_all_historical_data(trade_days_list, use_cache=True): st.stop()
            
        # 2. å¾ªç¯å›æµ‹
        bar = st.progress(0, text="å¯åŠ¨å¼•æ“...")
        for i, date in enumerate(dates_to_run):
            res, err = run_backtest_for_a_day(
                date, int(TOP_BACKTEST), 100, 
                MAX_PREV_PCT, RSI_LIMIT, 
                MAX_RET_5D, MAX_RET_10D # ä¼ å…¥ç†”æ–­å‚æ•°
            )
            
            if not res.empty:
                res['Trade_Date'] = date
                
                # å†™å…¥æ–­ç‚¹æ–‡ä»¶
                is_first = not os.path.exists(CHECKPOINT_FILE)
                res.to_csv(CHECKPOINT_FILE, mode='a', index=False, header=is_first, encoding='utf-8-sig')
                
                results.append(res)
            
            bar.progress((i+1)/len(dates_to_run), text=f"åˆ†æä¸­: {date}")
        
        bar.empty()
    
    # ç»“æœå±•ç¤º
    if results:
        all_res = pd.concat(results)
        
        # å®æ—¶è¿‡æ»¤ Top K
        all_res = all_res[all_res['Rank'] <= int(TOP_BACKTEST)]
        
        all_res['Trade_Date'] = all_res['Trade_Date'].astype(str)
        all_res = all_res.sort_values(['Trade_Date', 'Rank'], ascending=[False, True])
        
        st.header(f"ğŸ“Š V30.12.18 ç»Ÿè®¡ä»ªè¡¨ç›˜ (æ–¹æ¡ˆA: 5æ—¥<{MAX_RET_5D}% / 10æ—¥<{MAX_RET_10D}%)")
        cols = st.columns(3)
        for idx, n in enumerate([1, 3, 5]):
            col_name = f'Return_D{n} (%)'
            valid = all_res.dropna(subset=[col_name]) 
            if not valid.empty:
                avg = valid[col_name].mean()
                win = (valid[col_name] > 0).mean() * 100
                cols[idx].metric(f"D+{n} å‡ç›Š / èƒœç‡", f"{avg:.2f}% / {win:.1f}%")
        
        st.subheader("ğŸ“‹ å›æµ‹æ¸…å• (å«ç´¯è®¡æ¶¨å¹…)")
        
        show_cols = ['Rank', 'Trade_Date','name','ts_code','Close','Pct_Chg',
                     'pct_chg_5d', 'pct_chg_10d', # æ˜¾ç¤ºç´¯è®¡æ¶¨å¹…
                     'Return_D1 (%)','Return_D3 (%)','Return_D5 (%)',
                     'rsi','macd','market_state']
        
        # æ ¼å¼åŒ–æ˜¾ç¤º
        display_df = all_res[show_cols].copy()
        display_df = display_df.style.format({
            'Close': '{:.2f}', 'Pct_Chg': '{:.2f}%',
            'pct_chg_5d': '{:.2f}%', 'pct_chg_10d': '{:.2f}%',
            'Return_D1 (%)': '{:.2f}%', 'Return_D3 (%)': '{:.2f}%', 'Return_D5 (%)': '{:.2f}%',
            'rsi': '{:.2f}', 'macd': '{:.2f}'
        })
        
        st.dataframe(display_df, use_container_width=True)
        
        # ä¸‹è½½
        csv = all_res.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ CSV", csv, "export_v18.csv", "text/csv")
    else:
        st.warning("âš ï¸ æ²¡æœ‰é€‰å‡ºä»»ä½•è‚¡ç¥¨ï¼Œå¯èƒ½æ˜¯ç†”æ–­é˜ˆå€¼è®¾ç½®è¿‡ä½ï¼Ÿ")
