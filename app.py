# -*- coding: utf-8 -*-
"""
é€‰è‚¡ç‹ Â· V30.12.14 çœŸå®å†å²å›æµ‹ç‰ˆ
------------------------------------------------
ç‰ˆæœ¬å·ï¼šV30.12.14 (Tushare Pro ä¸“å±)
æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ã€çœŸÂ·å†å²å›æµ‹ã€‘ä½¿ç”¨ Tushare è·å–ç¾è‚¡ ETF å†å²æ—¥çº¿ã€‚
2. ã€åŠ¨æ€æ˜ å°„ã€‘æ¯ä¸€å¤©å›æµ‹éƒ½åŸºäºâ€œå½“æ—¶â€çš„ç¾è‚¡çƒ­ç‚¹ï¼ˆTop 3ï¼‰ã€‚
3. ã€ç²¾è‹±ç­–ç•¥ã€‘åšæŒ Rank 2 + é«˜å¼€å†²é«˜ä¹°å…¥ã€‚
------------------------------------------------
"""

import streamlit as st
import pandas as pd
import numpy as np
import tushare as ts
from datetime import datetime, timedelta
import warnings
import time
import concurrent.futures 
import os
import pickle

warnings.filterwarnings("ignore")

# ---------------------------
# 1. å…¨å±€é…ç½®
# ---------------------------
st.set_page_config(page_title="é€‰è‚¡ç‹ V30.12.14 çœŸå®å›æµ‹ç‰ˆ", layout="wide")

pro = None 
GLOBAL_ADJ_FACTOR = pd.DataFrame() 
GLOBAL_DAILY_RAW = pd.DataFrame() 
GLOBAL_QFQ_BASE_FACTORS = {} 
GLOBAL_STOCK_INDUSTRY = {} 
GLOBAL_US_DATA = pd.DataFrame() # å­˜å‚¨ç¾è‚¡ ETF å†å²æ•°æ®

# --- ç¾è‚¡ ETF ä»£ç è¡¨ (Tushare æ ¼å¼) ---
US_ETF_MAP = {
    'XLK.US': {'name': 'ç§‘æŠ€(XLK)', 'cn_inds': ['ç”µå­', 'è®¡ç®—æœº', 'é€šä¿¡']},
    'SOXX.US': {'name': 'åŠå¯¼ä½“(SOXX)', 'cn_inds': ['ç”µå­']}, 
    'XLC.US': {'name': 'é€šä¿¡(XLC)', 'cn_inds': ['ä¼ åª’', 'é€šä¿¡']},
    'XLV.US': {'name': 'åŒ»è¯(XLV)', 'cn_inds': ['åŒ»è¯ç”Ÿç‰©', 'ç¾å®¹æŠ¤ç†']},
    'XLY.US': {'name': 'å¯é€‰(XLY)', 'cn_inds': ['æ±½è½¦', 'å®¶ç”¨ç”µå™¨', 'å•†è´¸é›¶å”®', 'çººç»‡æœé¥°']},
    'XLP.US': {'name': 'å¿…é€‰(XLP)', 'cn_inds': ['é£Ÿå“é¥®æ–™', 'å†œæ—ç‰§æ¸”']},
    'XLE.US': {'name': 'èƒ½æº(XLE)', 'cn_inds': ['çŸ³æ²¹çŸ³åŒ–', 'ç…¤ç‚­']},
    'XLF.US': {'name': 'é‡‘è(XLF)', 'cn_inds': ['é“¶è¡Œ', 'éé“¶é‡‘è']},
    'XLI.US': {'name': 'å·¥ä¸š(XLI)', 'cn_inds': ['æœºæ¢°è®¾å¤‡', 'ç”µåŠ›è®¾å¤‡', 'å›½é˜²å†›å·¥', 'å»ºç­‘è£…é¥°']},
    'XLB.US': {'name': 'ææ–™(XLB)', 'cn_inds': ['æœ‰è‰²é‡‘å±', 'åŸºç¡€åŒ–å·¥', 'é’¢é“', 'å»ºç­‘ææ–™']},
    'XLRE.US': {'name': 'åœ°äº§(XLRE)', 'cn_inds': ['æˆ¿åœ°äº§']},
    'XLU.US': {'name': 'å…¬ç”¨(XLU)', 'cn_inds': ['å…¬ç”¨äº‹ä¸š', 'ç¯ä¿', 'ç”µåŠ›è®¾å¤‡']}
}
US_TICKERS = list(US_ETF_MAP.keys())

# --- ç”³ä¸‡ä¸€çº§è¡Œä¸šåˆ—è¡¨ ---
SW_INDUSTRIES = {
    '801010.SI': 'å†œæ—ç‰§æ¸”', '801030.SI': 'åŸºç¡€åŒ–å·¥', '801040.SI': 'é’¢é“',
    '801050.SI': 'æœ‰è‰²é‡‘å±', '801080.SI': 'ç”µå­', '801710.SI': 'å»ºç­‘ææ–™',
    '801720.SI': 'å»ºç­‘è£…é¥°', '801730.SI': 'ç”µåŠ›è®¾å¤‡', '801740.SI': 'å›½é˜²å†›å·¥',
    '801750.SI': 'è®¡ç®—æœº', '801760.SI': 'ä¼ åª’', '801770.SI': 'é€šä¿¡',
    '801880.SI': 'æ±½è½¦', '801890.SI': 'æœºæ¢°è®¾å¤‡', '801090.SI': 'äº¤è¿è®¾å¤‡', 
    '801110.SI': 'å®¶ç”¨ç”µå™¨', '801120.SI': 'é£Ÿå“é¥®æ–™', '801130.SI': 'çººç»‡æœé¥°',
    '801140.SI': 'è½»å·¥åˆ¶é€ ', '801150.SI': 'åŒ»è¯ç”Ÿç‰©', '801160.SI': 'å…¬ç”¨äº‹ä¸š',
    '801170.SI': 'äº¤é€šè¿è¾“', '801180.SI': 'æˆ¿åœ°äº§', '801200.SI': 'å•†è´¸é›¶å”®',
    '801210.SI': 'ç¤¾ä¼šæœåŠ¡', '801230.SI': 'ç»¼åˆ', '801780.SI': 'é“¶è¡Œ',
    '801790.SI': 'éé“¶é‡‘è', '801950.SI': 'ç…¤ç‚­', '801960.SI': 'çŸ³æ²¹çŸ³åŒ–',
    '801970.SI': 'ç¯ä¿', '801980.SI': 'ç¾å®¹æŠ¤ç†'
}
SW_NAMES_LIST = list(SW_INDUSTRIES.values())

# ---------------------------
# 2. åŸºç¡€å‡½æ•°
# ---------------------------
@st.cache_data(ttl=3600*12) 
def safe_get(func_name, **kwargs):
    global pro
    if pro is None: return pd.DataFrame(columns=['ts_code']) 
    func = getattr(pro, func_name) 
    try:
        for _ in range(3):
            try:
                # åªæœ‰ index_daily éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œus_daily ä¸éœ€è¦ is_index å‚æ•°
                if kwargs.get('is_index'): 
                    kwargs.pop('is_index')
                    df = pro.index_daily(**kwargs)
                else: 
                    df = func(**kwargs)
                
                if df is not None and not df.empty: return df
                time.sleep(0.5)
            except Exception as e:
                time.sleep(1)
                continue
        return pd.DataFrame(columns=['ts_code']) 
    except: return pd.DataFrame(columns=['ts_code'])

def get_trade_days(end_date_str, num_days):
    lookback_days = max(num_days * 3, 365) 
    start_date = (datetime.strptime(end_date_str, "%Y%m%d") - timedelta(days=lookback_days)).strftime("%Y%m%d")
    cal = safe_get('trade_cal', start_date=start_date, end_date=end_date_str)
    if cal.empty: return []
    trade_days_df = cal[cal['is_open'] == 1].sort_values('cal_date', ascending=False)
    return trade_days_df[trade_days_df['cal_date'] <= end_date_str]['cal_date'].head(num_days).tolist()

@st.cache_data(ttl=3600*24)
def fetch_and_cache_daily_data(date):
    adj = safe_get('adj_factor', trade_date=date)
    daily = safe_get('daily', trade_date=date)
    return {'adj': adj, 'daily': daily}

@st.cache_data(ttl=3600*24*7) 
def load_industry_mapping():
    global pro
    if pro is None: return {}
    try:
        all_members = []
        load_bar = st.progress(0, text="åŠ è½½è¡Œä¸šæ•°æ®...")
        codes = list(SW_INDUSTRIES.keys())
        for i, idx_code in enumerate(codes):
            df = pro.index_member(index_code=idx_code, is_new='Y')
            if not df.empty:
                df['industry_name'] = SW_INDUSTRIES[idx_code]
                all_members.append(df[['con_code', 'industry_name']])
            time.sleep(0.02)
            load_bar.progress((i + 1) / len(codes))
        load_bar.empty()
        if not all_members: return {}
        full_df = pd.concat(all_members)
        full_df = full_df.drop_duplicates(subset=['con_code'])
        return dict(zip(full_df['con_code'], full_df['industry_name']))
    except: return {}

# ---------------------------
# 3. ç¾è‚¡æ•°æ®å¤„ç† (æ–°å¢)
# ---------------------------
@st.cache_data(ttl=3600*12)
def get_us_history_data(start_date, end_date):
    """æ‰¹é‡è·å–ç¾è‚¡ ETF å†å²æ•°æ®å¹¶ç¼“å­˜"""
    global pro
    if pro is None: return pd.DataFrame()
    
    all_us_data = []
    # ç¨å¾®æ”¾å®½æ—¥æœŸèŒƒå›´ï¼Œç¡®ä¿èƒ½è¦†ç›–
    real_start = (datetime.strptime(start_date, "%Y%m%d") - timedelta(days=10)).strftime("%Y%m%d")
    
    # è¿›åº¦æ¡
    bar = st.progress(0, text="æ­£åœ¨ä¸‹è½½ç¾è‚¡ ETF å†å²æ•°æ® (Tushare)...")
    
    for i, ticker in enumerate(US_TICKERS):
        try:
            # Tushare ç¾è‚¡æ¥å£: us_daily
            df = safe_get('us_daily', ts_code=ticker, start_date=real_start, end_date=end_date)
            if not df.empty:
                df = df[['trade_date', 'ts_code', 'pct_change']] # ä¿ç•™æ¶¨è·Œå¹…
                all_us_data.append(df)
        except: pass
        bar.progress((i+1)/len(US_TICKERS))
    bar.empty()
    
    if not all_us_data: return pd.DataFrame()
    
    # åˆå¹¶å¹¶é‡æ„ï¼šIndex=Date, Columns=ETF, Value=Pct_Change
    full_df = pd.concat(all_us_data)
    # ç¡®ä¿æ—¥æœŸæ ¼å¼ç»Ÿä¸€
    full_df['trade_date'] = pd.to_datetime(full_df['trade_date']).dt.strftime('%Y%m%d')
    pivot_df = full_df.pivot(index='trade_date', columns='ts_code', values='pct_change')
    return pivot_df.sort_index()

def get_hot_inds_for_date(trade_date_str, top_n=3):
    """æ ¹æ® A è‚¡äº¤æ˜“æ—¥ï¼ŒæŸ¥æ‰¾ä¹‹å‰æœ€è¿‘ä¸€ä¸ªç¾è‚¡äº¤æ˜“æ—¥çš„çƒ­ç‚¹"""
    global GLOBAL_US_DATA
    if GLOBAL_US_DATA.empty: return []
    
    # æ‰¾åˆ°æ‰€æœ‰å°äº trade_date_str çš„ç¾è‚¡æ—¥æœŸ
    valid_dates = GLOBAL_US_DATA.index[GLOBAL_US_DATA.index < trade_date_str]
    if len(valid_dates) == 0: return []
    
    # å–æœ€è¿‘çš„ä¸€å¤©
    target_us_date = valid_dates[-1]
    
    # è·å–å½“å¤©çš„æ¶¨è·Œå¹…æ•°æ®
    row = GLOBAL_US_DATA.loc[target_us_date].dropna()
    if row.empty: return []
    
    # æ’åºå–å‰ N å
    top_etfs = row.sort_values(ascending=False).head(top_n)
    
    # æ˜ å°„åˆ° A è‚¡è¡Œä¸š
    target_inds = set()
    for etf_code, pct in top_etfs.items():
        if pct > 0: # åªæœ‰æ¶¨çš„æ‰ç®—çƒ­ç‚¹ï¼Ÿæˆ–è€…ä¸ç®¡æ¶¨è·Œåªè¦æ’åé å‰ï¼Ÿå»ºè®®åªçœ‹æ¶¨çš„ï¼Œæˆ–è€… Top3
            info = US_ETF_MAP.get(etf_code)
            if info:
                target_inds.update(info['cn_inds'])
                
    return list(target_inds)

# ---------------------------
# 4. æ•°æ®ä¸‹è½½ä¸ç¼“å­˜é€»è¾‘
# ---------------------------
CACHE_FILE_NAME = "market_data_cache_v14.pkl"

def get_all_historical_data(trade_days_list, use_cache=True):
    global GLOBAL_ADJ_FACTOR, GLOBAL_DAILY_RAW, GLOBAL_QFQ_BASE_FACTORS, GLOBAL_STOCK_INDUSTRY, GLOBAL_US_DATA
    
    GLOBAL_STOCK_INDUSTRY = load_industry_mapping()

    if use_cache and os.path.exists(CACHE_FILE_NAME):
        st.success("âš¡ æé€ŸåŠ è½½æœ¬åœ°ç¼“å­˜...")
        try:
            with open(CACHE_FILE_NAME, 'rb') as f:
                d = pickle.load(f)
                GLOBAL_ADJ_FACTOR = d['adj']
                GLOBAL_DAILY_RAW = d['daily']
                GLOBAL_US_DATA = d.get('us_data', pd.DataFrame()) # æ–°å¢ç¾è‚¡ç¼“å­˜
            
            latest = GLOBAL_ADJ_FACTOR.index.get_level_values('trade_date').max()
            if latest:
                try: GLOBAL_QFQ_BASE_FACTORS = GLOBAL_ADJ_FACTOR.loc[(slice(None), latest), 'adj_factor'].droplevel(1).to_dict()
                except: pass
            
            # å¦‚æœç¼“å­˜é‡Œæ²¡æœ‰ç¾è‚¡æ•°æ®ï¼Œéœ€è¦é‡æ–°ä¸‹è½½
            if GLOBAL_US_DATA.empty:
                st.warning("ç¼“å­˜ä¸­ç¼ºå°‘ç¾è‚¡æ•°æ®ï¼Œæ­£åœ¨è¡¥å……ä¸‹è½½...")
                start_d = min(trade_days_list)
                end_d = max(trade_days_list)
                GLOBAL_US_DATA = get_us_history_data(start_d, end_d)
                # æ›´æ–°ç¼“å­˜
                with open(CACHE_FILE_NAME, 'wb') as f: 
                    pickle.dump({'adj': GLOBAL_ADJ_FACTOR, 'daily': GLOBAL_DAILY_RAW, 'us_data': GLOBAL_US_DATA}, f)
            
            return True
        except: 
            os.remove(CACHE_FILE_NAME)

    # ä¸‹è½½æ–°æ•°æ®
    latest = max(trade_days_list) 
    earliest = min(trade_days_list)
    s_dt = datetime.strptime(earliest, "%Y%m%d") - timedelta(days=200)
    e_dt = datetime.strptime(latest, "%Y%m%d") + timedelta(days=30)
    start_date = s_dt.strftime("%Y%m%d")
    end_date = e_dt.strftime("%Y%m%d")
    
    # 1. ä¸‹è½½ A è‚¡æ•°æ®
    cal = safe_get('trade_cal', start_date=start_date, end_date=end_date, is_open='1')
    if cal.empty: return False
    all_dates = cal['cal_date'].tolist()
    
    st.info(f"ğŸ“¡ æ­£åœ¨ä¸‹è½½æ•°æ® (Aè‚¡ + ç¾è‚¡)...")
    
    # 2. å¹¶è¡Œä¸‹è½½ A è‚¡
    adj_list, daily_list = [], []
    def fetch_worker(date): return fetch_and_cache_daily_data(date)
    
    bar = st.progress(0)
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        f2d = {executor.submit(fetch_worker, d): d for d in all_dates}
        for i, f in enumerate(concurrent.futures.as_completed(f2d)):
            try:
                d = f.result()
                if not d['adj'].empty: adj_list.append(d['adj'])
                if not d['daily'].empty: daily_list.append(d['daily'])
            except: pass
            if i%10==0: bar.progress((i+1)/len(all_dates))
    bar.empty()
    
    # 3. ä¸‹è½½ç¾è‚¡æ•°æ®
    GLOBAL_US_DATA = get_us_history_data(start_date, end_date)
    
    if not daily_list: return False
    with st.spinner("æ„å»ºç´¢å¼•..."):
        GLOBAL_ADJ_FACTOR = pd.concat(adj_list).drop_duplicates(subset=['ts_code', 'trade_date']).set_index(['ts_code', 'trade_date']).sort_index()
        GLOBAL_ADJ_FACTOR['adj_factor'] = pd.to_numeric(GLOBAL_ADJ_FACTOR['adj_factor'], errors='coerce').fillna(0)
        GLOBAL_DAILY_RAW = pd.concat(daily_list).drop_duplicates(subset=['ts_code', 'trade_date']).set_index(['ts_code', 'trade_date']).sort_index()
        
        latest_d = GLOBAL_ADJ_FACTOR.index.get_level_values('trade_date').max()
        if latest_d:
            try: GLOBAL_QFQ_BASE_FACTORS = GLOBAL_ADJ_FACTOR.loc[(slice(None), latest_d), 'adj_factor'].droplevel(1).to_dict()
            except: pass
            
        try:
            with open(CACHE_FILE_NAME, 'wb') as f: 
                pickle.dump({'adj': GLOBAL_ADJ_FACTOR, 'daily': GLOBAL_DAILY_RAW, 'us_data': GLOBAL_US_DATA}, f)
        except: pass
    return True

# ---------------------------
# 5. æ ¸å¿ƒè®¡ç®—
# ---------------------------
def get_qfq_data_v4_optimized_final(ts_code, start_date, end_date):
    global GLOBAL_DAILY_RAW, GLOBAL_ADJ_FACTOR, GLOBAL_QFQ_BASE_FACTORS
    if GLOBAL_DAILY_RAW.empty: return pd.DataFrame()
    latest_adj = GLOBAL_QFQ_BASE_FACTORS.get(ts_code, np.nan)
    if pd.isna(latest_adj): return pd.DataFrame()
    try:
        d = GLOBAL_DAILY_RAW.loc[ts_code]
        d = d[(d.index >= start_date) & (d.index <= end_date)]
        a = GLOBAL_ADJ_FACTOR.loc[ts_code]['adj_factor']
        a = a[(a.index >= start_date) & (a.index <= end_date)]
    except: return pd.DataFrame()
    if d.empty or a.empty: return pd.DataFrame()
    df = d.merge(a.rename('adj_factor'), left_index=True, right_index=True, how='left').dropna()
    for c in ['open','high','low','close']: df[c+'_qfq'] = df[c] * df['adj_factor'] / latest_adj
    df = df.reset_index().rename(columns={'trade_date':'trade_date_str'}).sort_values('trade_date_str').set_index('trade_date_str')
    for c in ['open','high','low','close']: df[c] = df[c+'_qfq']
    return df[['open','high','low','close','vol']]

def get_future_prices(ts_code, selection_date, d0_qfq_close, days_ahead=[1, 3, 5]):
    d0 = datetime.strptime(selection_date, "%Y%m%d")
    s = (d0 + timedelta(days=1)).strftime("%Y%m%d")
    e = (d0 + timedelta(days=15)).strftime("%Y%m%d")
    h = get_qfq_data_v4_optimized_final(ts_code, start_date=s, end_date=e)
    res = {}
    if h.empty: return res
    h['open'] = pd.to_numeric(h['open'], errors='coerce')
    h['high'] = pd.to_numeric(h['high'], errors='coerce')
    h['close'] = pd.to_numeric(h['close'], errors='coerce')
    
    d1 = h.iloc[0]
    next_open = d1['open']
    next_high = d1['high']
    
    # ç²¾è‹±ä¹°å…¥æ¡ä»¶
    if next_open <= d0_qfq_close: return res
    target_buy = next_open * 1.015
    if next_high < target_buy: return res
    
    for n in days_ahead:
        col = f'Return_D{n}'
        if len(h) >= n:
            res[col] = (h.iloc[n-1]['close'] - target_buy)/target_buy * 100
    return res

def compute_indicators(ts_code, end_date):
    s = (datetime.strptime(end_date, "%Y%m%d") - timedelta(days=150)).strftime("%Y%m%d")
    df = get_qfq_data_v4_optimized_final(ts_code, s, end_date)
    if df.empty or len(df)<26: return {}
    df['pct'] = df['close'].pct_change().fillna(0)*100
    res = {'last_close': df['close'].iloc[-1], 'last_high': df['high'].iloc[-1]}
    
    ema12 = df['close'].ewm(span=12).mean()
    ema26 = df['close'].ewm(span=26).mean()
    diff = ema12 - ema26
    dea = diff.ewm(span=9).mean()
    res['macd_val'] = ((diff-dea)*2).iloc[-1]
    
    ma20 = df['close'].tail(20).mean()
    res['ma20'] = ma20
    if ma20 > 0: res['bias_20'] = (res['last_close']-ma20)/ma20*100
    else: res['bias_20'] = 0
    
    delta = df['close'].diff()
    gain = (delta.where(delta>0, 0)).ewm(alpha=1/12).mean()
    loss = (-delta.where(delta<0, 0)).ewm(alpha=1/12).mean()
    rs = gain/(loss+1e-9)
    res['rsi_12'] = 100 - (100/(1+rs)).iloc[-1]
    return res

def get_market_state(trade_date):
    s = (datetime.strptime(trade_date, "%Y%m%d") - timedelta(days=40)).strftime("%Y%m%d")
    i = safe_get('daily', ts_code='000300.SH', start_date=s, end_date=trade_date, is_index=True)
    if i.empty or len(i)<20: return 'Weak'
    return 'Strong' if i.iloc[-1]['close'] > i['close'].tail(20).mean() else 'Weak'

# ---------------------------
# 6. å›æµ‹ä¸»é€»è¾‘ (é›†æˆç¾è‚¡å†å²)
# ---------------------------
def run_backtest_for_a_day(last_trade, TOP_BACKTEST, FINAL_POOL, MAX_UPPER_SHADOW, MAX_TURNOVER_RATE, RSI_LIMIT, CHIP_MIN_WIN_RATE, SECTOR_THRESHOLD, MIN_MV, MAX_MV, MAX_PREV_PCT, MIN_PRICE, USE_US_MAP, TOP_N_US):
    global GLOBAL_STOCK_INDUSTRY
    
    # åŠ¨æ€è·å–å½“å¤©çš„ç¾è‚¡æ˜ å°„è¡Œä¸š
    target_inds = []
    if USE_US_MAP:
        target_inds = get_hot_inds_for_date(last_trade, top_n=TOP_N_US)
    
    market_state = get_market_state(last_trade)
    daily = safe_get('daily', trade_date=last_trade)
    if daily.empty: return pd.DataFrame(), "æ— æ•°æ®"
    
    base = safe_get('stock_basic', list_status='L')
    df = daily.merge(base[['ts_code','name']], on='ts_code')
    
    daily_basic = safe_get('daily_basic', trade_date=last_trade)
    if not daily_basic.empty:
        df = df.merge(daily_basic[['ts_code','turnover_rate','circ_mv']], on='ts_code', how='left')
    else: df['turnover_rate'] = 0; df['circ_mv'] = 0
        
    mf = safe_get('moneyflow', trade_date=last_trade)
    if not mf.empty: df = df.merge(mf[['ts_code','net_mf_amount']], on='ts_code', how='left').rename(columns={'net_mf_amount':'net_mf'})
    else: df['net_mf'] = 0
    df['net_mf'] = df['net_mf'].fillna(0)
    
    df = df[~df['name'].str.contains('ST|é€€')]
    df = df[(df['close']>=MIN_PRICE) & (df['close']<=2000) & (df['turnover_rate']<=MAX_TURNOVER_RATE)]
    df = df[(df['circ_mv']/10000 >= MIN_MV) & (df['circ_mv']/10000 <= MAX_MV)]
    
    if df.empty: return pd.DataFrame(), "è¿‡æ»¤ç©º"
    cands = df.sort_values('pct_chg', ascending=False).head(FINAL_POOL)
    recs = []
    
    for row in cands.itertuples():
        # ç¾è‚¡å†å²æ˜ å°„é€»è¾‘
        ind_name = GLOBAL_STOCK_INDUSTRY.get(row.ts_code)
        if USE_US_MAP:
            # å¦‚æœå¼€å¯äº†æ˜ å°„ï¼Œä½†å½“å¤©ç¾è‚¡æ²¡æ•°æ®ï¼ˆæ¯”å¦‚ç¾è‚¡ä¼‘å¸‚ï¼‰ï¼Œåˆ™è·³è¿‡æ˜ å°„é™åˆ¶ï¼Ÿè¿˜æ˜¯ä¸¥æ ¼æ‰§è¡Œï¼Ÿ
            # å»ºè®®ï¼šå¦‚æœ target_inds ä¸ºç©ºï¼ˆæ²¡æ•°æ®ï¼‰ï¼Œåˆ™é™çº§ä¸ºä¸é™åˆ¶ï¼Œæˆ–è€…è·³è¿‡ã€‚
            # è¿™é‡Œé€»è¾‘ï¼šå¦‚æœ target_inds æœ‰å€¼ï¼Œåˆ™ä¸¥æ ¼è¿‡æ»¤ï¼›å¦‚æœæ²¡å€¼ï¼Œåˆ™ä¸é™åˆ¶ã€‚
            if target_inds and (ind_name not in target_inds): continue
        
        if row.pct_chg > MAX_PREV_PCT: continue
        
        ind = compute_indicators(row.ts_code, last_trade)
        if not ind: continue
        
        if ind.get('bias_20', 0) > 20: continue
        if ind['last_close'] < ind['ma20']: continue
        if market_state == 'Weak' and ind['rsi_12'] > RSI_LIMIT: continue
        
        upper = (ind['last_high'] - ind['last_close'])/ind['last_close']*100
        if upper > MAX_UPPER_SHADOW: continue
        
        fut = get_future_prices(row.ts_code, last_trade, ind['last_close'])
        
        recs.append({
            'ts_code': row.ts_code, 'name': row.name, 'rsi': ind['rsi_12'],
            'macd': ind['macd_val'], 'net_mf': row.net_mf, 'bias_20': ind.get('bias_20',0),
            'Return_D1 (%)': fut.get('Return_D1'), 'Return_D3 (%)': fut.get('Return_D3'),
            'Return_D5 (%)': fut.get('Return_D5'), 'market_state': market_state,
            'winner_rate': 80,
            'us_link': 'Yes' if (USE_US_MAP and target_inds) else 'No'
        })

    if not recs: return pd.DataFrame(), "æ— æ ‡çš„"
    fdf = pd.DataFrame(recs)
    
    def score(r):
        s = r['macd']*1000 + r['net_mf']/10000
        if 60<=r['rsi']<=85: s+=2000
        elif r['rsi']>90: s-=1000
        if r['bias_20']<10: s+=1000
        if r['market_state']=='Strong' and r['rsi']>RSI_LIMIT: s-=500
        return s
    
    fdf['Score'] = fdf.apply(score, axis=1)
    final = fdf.sort_values('Score', ascending=False).head(TOP_BACKTEST)
    final.insert(0, 'Rank', range(1, len(final)+1))
    return final, None

# ---------------------------
# 7. UI
# ---------------------------
with st.sidebar:
    st.header("V30.12.14 çœŸå®å›æµ‹ç‰ˆ")
    backtest_date_end = st.date_input("åˆ†ææˆªæ­¢æ—¥æœŸ", value=datetime.now().date())
    BACKTEST_DAYS = st.number_input("åˆ†æå¤©æ•°", value=30, step=1)
    TOP_BACKTEST = st.number_input("æ¯æ—¥ä¼˜é€‰ TopK", value=5)
    
    st.markdown("---")
    st.subheader("ğŸ‡ºğŸ‡¸ å†å²çœŸå®è”åŠ¨")
    USE_US_MAP = st.checkbox("å¼€å¯ç¾è‚¡å†å²å›æº¯", value=True)
    TOP_N_US = st.slider("é€‰å–ç¾è‚¡ Top N æ¿å—", 1, 5, 3) # é»˜è®¤ Top 3
    
    st.markdown("---")
    RESUME_CHECKPOINT = st.checkbox("ğŸ”¥ å¼€å¯æ–­ç‚¹ç»­ä¼ ", value=True)
    if st.button("ğŸ—‘ï¸ æ¸…é™¤è¡Œæƒ…ç¼“å­˜"):
        if os.path.exists(CACHE_FILE_NAME): os.remove(CACHE_FILE_NAME)
    CHECKPOINT_FILE = "backtest_checkpoint_v14.csv"
    
    # åŸºç¡€å‚æ•°
    MIN_PRICE = st.number_input("æœ€ä½è‚¡ä»·", value=10.0) 
    MIN_MV = st.number_input("æœ€å°å¸‚å€¼(äº¿)", value=30.0) 
    MAX_MV = st.number_input("æœ€å¤§å¸‚å€¼(äº¿)", value=1000.0)
    CHIP_MIN_WIN_RATE = st.number_input("æœ€ä½è·åˆ©ç›˜ (%)", value=70.0)
    MAX_PREV_PCT = st.number_input("æ˜¨æ—¥æœ€å¤§æ¶¨å¹…é™åˆ¶ (%)", value=19.0)
    RSI_LIMIT = st.number_input("RSI æ‹¦æˆªçº¿", value=100.0)
    SECTOR_THRESHOLD = st.number_input("æ¿å—æ¶¨å¹… (%)", value=1.5)
    MAX_UPPER_SHADOW = st.number_input("ä¸Šå½±çº¿ (%)", value=5.0)
    MAX_TURNOVER_RATE = st.number_input("æ¢æ‰‹ç‡ (%)", value=20.0)

TS_TOKEN = st.text_input("Tushare Token", type="password")
if not TS_TOKEN: st.stop()
ts.set_token(TS_TOKEN)
pro = ts.pro_api()

if st.button(f"ğŸš€ å¯åŠ¨çœŸå®å›æµ‹"):
    processed_dates = set()
    results = []
    
    if RESUME_CHECKPOINT and os.path.exists(CHECKPOINT_FILE):
        try:
            existing_df = pd.read_csv(CHECKPOINT_FILE)
            existing_df['Trade_Date'] = existing_df['Trade_Date'].astype(str)
            processed_dates = set(existing_df['Trade_Date'].unique())
            results.append(existing_df)
            st.success(f"âœ… æ–­ç‚¹ç»­ä¼ ï¼šè·³è¿‡ {len(processed_dates)} å¤©")
        except: pass
    else:
        if os.path.exists(CHECKPOINT_FILE): os.remove(CHECKPOINT_FILE)
    
    trade_days_list = get_trade_days(backtest_date_end.strftime("%Y%m%d"), int(BACKTEST_DAYS))
    if not trade_days_list: st.stop()
        
    dates_to_run = [d for d in trade_days_list if d not in processed_dates]
    
    if not dates_to_run:
        st.success("ğŸ‰ åˆ†æå®Œæ¯•")
    else:
        # ä¸‹è½½ Aè‚¡ + ç¾è‚¡å†å²æ•°æ®
        if not get_all_historical_data(trade_days_list, use_cache=True): st.stop()
            
        bar = st.progress(0, text="å¯åŠ¨å¼•æ“...")
        for i, date in enumerate(dates_to_run):
            # ä¼ å…¥ USE_US_MAP å’Œ TOP_N_US
            res, err = run_backtest_for_a_day(
                date, int(TOP_BACKTEST), 100, MAX_UPPER_SHADOW, MAX_TURNOVER_RATE, 
                RSI_LIMIT, CHIP_MIN_WIN_RATE, SECTOR_THRESHOLD, MIN_MV, MAX_MV, 
                MAX_PREV_PCT, MIN_PRICE, 
                USE_US_MAP, TOP_N_US
            )
            if not res.empty:
                res['Trade_Date'] = date
                is_first = not os.path.exists(CHECKPOINT_FILE)
                res.to_csv(CHECKPOINT_FILE, mode='a', index=False, header=is_first, encoding='utf-8-sig')
                results.append(res)
            bar.progress((i+1)/len(dates_to_run), text=f"åˆ†æä¸­: {date}")
        bar.empty()
    
    if results:
        all_res = pd.concat(results)
        all_res = all_res[all_res['Rank'] <= int(TOP_BACKTEST)]
        all_res['Trade_Date'] = all_res['Trade_Date'].astype(str)
        all_res = all_res.sort_values(['Trade_Date', 'Rank'], ascending=[False, True])
        
        st.header(f"ğŸ“Š ç»Ÿè®¡ä»ªè¡¨ç›˜ (å†å²çœŸå®å›æµ‹ - Top{TOP_N_US})")
        cols = st.columns(3)
        for idx, n in enumerate([1, 3, 5]):
            col_name = f'Return_D{n} (%)'
            valid = all_res.dropna(subset=[col_name]) 
            if not valid.empty:
                avg = valid[col_name].mean()
                win = (valid[col_name] > 0).mean() * 100
                cols[idx].metric(f"D+{n} å‡ç›Š / èƒœç‡", f"{avg:.2f}% / {win:.1f}%")
        
        st.dataframe(all_res, use_container_width=True)
        csv = all_res.to_csv(index=False).encode('utf-8-sig')
        st.download_button("ğŸ“¥ ä¸‹è½½ç»“æœ", csv, f"export_v14_real.csv", "text/csv")
    else:
        st.warning("âš ï¸ æ²¡æœ‰ç»“æœ")
